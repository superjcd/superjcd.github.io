<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>数据库 on superjcd</title><link>https://superjcd.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/</link><description>Recent content in 数据库 on superjcd</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 29 Dec 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://superjcd.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/index.xml" rel="self" type="application/rss+xml"/><item><title>Database Internal阅读笔记(上)</title><link>https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/</link><pubDate>Fri, 29 Dec 2023 00:00:00 +0000</pubDate><guid>https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/</guid><description>&lt;h2 id="第二章-b-tree-basics">第二章： B-Tree Basics
&lt;/h2>&lt;ul>
&lt;li>B+ 树是几乎所有基于磁盘的关系型数据库(mysql， pg等)的基础数据结构&lt;/li>
&lt;li>B+ 树是Binary Search Tree的衍生， 有跟多的fanout, 所以搜索速度会更快（Log2 -&amp;gt; LogK, 当然在它们的算法复杂度是一致的）&lt;/li>
&lt;li>B+树的搜索见下图， 是从root根节点一直往下遍历的&lt;/li>
&lt;li>数据的增删会触发B+数的split和merge（触发条件就是overflow和underflow）&lt;/li>
&lt;/ul>
&lt;h4 id="示意图">示意图
&lt;/h4>&lt;p>B+树（n key， n+1 pointer结构， 当然有些B+树只会在叶子节点存储数据， 然后子节点之间用链表和双向链表来提升查询效率）.&lt;/p>
&lt;p>&lt;img src="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/1.png"
width="532"
height="151"
srcset="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/1_hu11496292715019633274.png 480w, https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/1_hu14933288381283139119.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="352"
data-flex-basis="845px"
>&lt;/p>
&lt;p>leaf node的split.&lt;/p>
&lt;p>&lt;img src="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/2.png"
width="674"
height="206"
srcset="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/2_hu16048913461160528288.png 480w, https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/2_hu15129793064902286847.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="327"
data-flex-basis="785px"
>&lt;/p>
&lt;p>leaf node的merge&lt;/p>
&lt;p>&lt;img src="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/3.png"
width="614"
height="197"
srcset="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/3_hu15680180638441770598.png 480w, https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/3_hu8146204659701067482.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="311"
data-flex-basis="748px"
>&lt;/p>
&lt;p>non-leaf node的split&lt;/p>
&lt;p>&lt;img src="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/4.png"
width="684"
height="229"
srcset="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/4_hu903176938407582279.png 480w, https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/4_hu225077400272659496.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="298"
data-flex-basis="716px"
>&lt;/p>
&lt;p>non-leaf node的merge
&lt;img src="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/5.png"
width="677"
height="246"
srcset="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/5_hu11568399917259774228.png 480w, https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/5_hu7008973578993771967.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="275"
data-flex-basis="660px"
>&lt;/p>
&lt;p>上面也看到了， 树的merge和split是比较耗时的操作(以split为例， 在某些场景下是需要一直向上递归的)， 特别在使用磁盘的场景下， 那么减少这种操作（调度）， 或者该表数据大小（压缩）通常会是优化数据库性能的重要方向&lt;/p>
&lt;h2 id="第三章-file-formats">第三章 File Formats
&lt;/h2>&lt;p>基于磁盘存储的数据库， 它的主要的存储单元是Page, 大小一般是4kb-16kb之间， 基本上一个Page对应一个B-Tree的节点&lt;/p>
&lt;h3 id="原始的page结构">原始的Page结构
&lt;/h3>&lt;p>&lt;img src="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/6.png"
width="642"
height="84"
srcset="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/6_hu5428297086272304495.png 480w, https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/6_hu12019344167207151415.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="764"
data-flex-basis="1834px"
>&lt;/p>
&lt;ul>
&lt;li>p： 指针， 指向子节点&lt;/li>
&lt;li>k: key&lt;/li>
&lt;li>v： value&lt;/li>
&lt;/ul>
&lt;p>这种结构非常适合用来存储大小固定的数据， 比如char(13)来存储电话号码(那么所有电话号码都是同样长度的， 这种方式的存储和查找非常的便利)
但是这种方式最大的问题是不好存储存储可变长度的数据， 比如string或者text&lt;/p>
&lt;h3 id="slotted-page">slotted page
&lt;/h3>&lt;p>现代数据库的page结构， 主要采用的是slotted page的形式：
&lt;img src="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/7.png"
width="458"
height="280"
srcset="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/7_hu12346393113962113822.png 480w, https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/7_hu9985965968662141495.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="163"
data-flex-basis="392px"
>&lt;br>
好处就是解决了原始Page结构不好存储可变长度记录的问题， 同时支持在数据被删除的时候进行更好的再利用（下面会提及）&lt;/p>
&lt;h3 id="cell">Cell
&lt;/h3>&lt;p>cell存储各种类型的数据，它在Slotted Page中的插入方式是从尾部往前插入， 然后在Header中的Poitner会依次保留各个cell的offet信息&lt;/p>
&lt;p>&lt;img src="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/8.png"
width="628"
height="140"
srcset="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/8_hu11445287324942386616.png 480w, https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/8_hu12426533443688908740.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="448"
data-flex-basis="1076px"
>&lt;/p>
&lt;blockquote>
&lt;p>为什么采用这样的插入顺序其实也很好理解， 如果直接从左至右， 每次有一个新的cell, 那么原来存储在最左侧（紧挨着Header）的cell， 由于Header变化了， 它也需要相应地右移， 而从尾部向前则不需要&lt;/p>
&lt;/blockquote>
&lt;p>当然如果要保持Offset的逻辑顺序(比如根据Cell数据的字母序)， 我们可以调整Offset的位置
&lt;img src="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/9.png"
width="640"
height="156"
srcset="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/9_hu8837436741558694158.png 480w, https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/9_hu15834357353017269192.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="410"
data-flex-basis="984px"
>&lt;/p>
&lt;blockquote>
&lt;p>插入顺序： Tom -&amp;gt; Lesile -&amp;gt; Ron， Offsets的顺序则是Lesile -&amp;gt; Ron -&amp;gt; Tom&lt;/p>
&lt;/blockquote>
&lt;p>前面提到了， Slotted Page在空间再利用（reclaim space， 利用被删除的数据）的高效性， 以sqlite db为例， 数据库会维持一份可以用space的指针列表.
&lt;img src="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/10.png"
width="663"
height="210"
srcset="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/10_hu13441783873922228685.png 480w, https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/10_hu9216312680131979259.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="315"
data-flex-basis="757px"
>&lt;br>
当要插入新的数据的时候， 会使用First fit或者Best fit算法去找到哪一块空间最最适合插入新数据&lt;/p>
&lt;blockquote>
&lt;p>First fit: 找到第一块可以容纳新数据的space
Best fit：找到浪费最小的space(就是space和新插入数据大小的差值最小)
从空间利用率上Best fit会更好，当然First fit速度明显会更快&lt;/p>
&lt;/blockquote>
&lt;p>假设， 没有办法找到任意一块能够容纳新数据的空白space， 并且我的空白space总和又大于新数据， 那这个时候就需要进行去碎片化(defragmenting)&lt;/p>
&lt;h3 id="cell的结构">Cell的结构
&lt;/h3>&lt;p>cell有两种， 第一种叫做Key Cell， 主要用来存储key。第二种是Key-Value Cell, 就是同时用来存储key和vlaue的， 一个page上面， 通常是这有这两种key中的任意一种的(因为两种key的结构略有不同， 所以从效率角度， 肯定是存储一种类型的会更好).&lt;/p>
&lt;p>Key Cell结构：&lt;/p>
&lt;ul>
&lt;li>cell type&lt;/li>
&lt;li>key size&lt;/li>
&lt;li>child page id&lt;/li>
&lt;li>key bytes&lt;/li>
&lt;/ul>
&lt;p>Key-Value Cell结构:&lt;/p>
&lt;ul>
&lt;li>cell type&lt;/li>
&lt;li>key size&lt;/li>
&lt;li>value size&lt;/li>
&lt;li>key bytes&lt;/li>
&lt;li>Data record bytes(也就是value bytes)&lt;/li>
&lt;/ul>
&lt;h2 id="第四章-implementing-b-trees">第四章 Implementing B-Trees
&lt;/h2>&lt;h3 id="page-header">Page Header
&lt;/h3>&lt;p>page header会存储一些和page相关的元信息， 除了上前面提到的cell offsets, 还会有诸如：&lt;/p>
&lt;ul>
&lt;li>Magic numbers&lt;/li>
&lt;li>sibling links&lt;/li>
&lt;li>rightmost pointers&lt;/li>
&lt;li>node high key&lt;/li>
&lt;/ul>
&lt;p>magic numbers是会存储在Page上的的特定位置的二进制的值， 比如存储第51 ， 47， 41， 45位的值， 目的是为了验证page有没有被污染， 只要在读数据的时候， 比较一下这几位的实际值和存储在header中的值是否一样即可
一般来说每个node， 会存储n个key以及n+1 pointer指向这个node的子节点， 这个第n个key经常会被存储在page header
&lt;img src="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/11.png"
width="699"
height="237"
srcset="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/11_hu16097888266148504105.png 480w, https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/11_hu1859240776017745930.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="294"
data-flex-basis="707px"
>&lt;/p>
&lt;p>有时候， 这种方式会有一种变体就是key会饿pointer的数量对齐：
&lt;img src="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/12.png"
width="298"
height="184"
srcset="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/12_hu5296876375901662597.png 480w, https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/12_hu17008141340834417729.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="161"
data-flex-basis="388px"
>&lt;br>
上面的K3就是high key&lt;/p>
&lt;h3 id="overflow-pages">Overflow pages
&lt;/h3>&lt;p>通常page的size是固定的， 在4-16kb之间， 过大的size不可避免的会照成空间的浪费。 但是page size的固定话， 当可变长的数据过大的时候，我们就需要有一个扩展的页去存储溢出的数据, 这些页就被称为overflow page.&lt;/p>
&lt;blockquote>
&lt;p>说到底， 磁盘的load reload的开销过大才需要固定size的page， 以及这种overflow page， 如果是内存， 之间原来的size 按比例扩容就好了&lt;/p>
&lt;/blockquote>
&lt;p>page header会保留指向这些overflow page的指针（overflow page也需要保留指向原始页面的指针）
&lt;img src="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/13.png"
width="425"
height="367"
srcset="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/13_hu15914841279285001264.png 480w, https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/13_hu5818170485353002403.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="115"
data-flex-basis="277px"
>&lt;br>
一般来说说， 一个page只会指向单一的overflow page， 如果还是放不下， 那么可以继续从overlow page出发链接新的overflow page&lt;/p>
&lt;h3 id="二分查找在页面中应用">二分查找在页面中应用
&lt;/h3>&lt;p>由于page header中可以以逻辑顺序保留cell的offsets， 那么我们完全可以对这些offset进行二分查找：
&lt;img src="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/14.png"
width="666"
height="254"
srcset="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/14_hu4636528305755054932.png 480w, https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/14_hu3022970279502262825.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="262"
data-flex-basis="629px"
>&lt;br>
具体步骤就是， 现取这些offsets中最中间的那个， 然后找到对应的cell， 比较和查找键的大小来决定二分查找的方向是往左还是往右&lt;/p>
&lt;h3 id="页的split和merge的传播">页的Split和Merge的传播
&lt;/h3>&lt;p>随着数据的插入和删除， B树的节点需要进行相应的拆分 &amp;amp; 合并， 这意味着子节点的父节点以也会发生变化， 而且这种变化通常是一层层向上传播的。有没有一种高效的算法去更新这种变化 ， 其中一种就是Breadcrums（面包屑）， 这个算法的名称其实很形象地指出了它的实现方式：
我们查找新的插入节点， 一定是从根节点之上而下的， 如果我们记录了这个path的话， 那么将其逆序， 不就找到了每个子节点的父节点嘛？
&lt;img src="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/15.png"
width="608"
height="454"
srcset="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/15_hu10514125994058762311.png 480w, https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/15_hu16135562310550821172.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="133"
data-flex-basis="321px"
>&lt;/p>
&lt;blockquote>
&lt;p>比如我们要插入一个大于37的数值， 查找路径入上入所示， Breadcrumbs记录了每个insertion point的offset。 Breadcumbs的实现一般是stack&lt;/p>
&lt;/blockquote>
&lt;h3 id="rebalancing">Rebalancing
&lt;/h3>&lt;p>通常来讲， 树的再平衡会涉及到大量的split和merge， 进而影响数据库性能。所以数据库通常会延迟split和merge， 并使用一种相对开销较少的方式去实现reblancing &amp;ndash; 平衡邻居节点之间的数据， 把有较多数据的节点的数据搬到隔壁的节点， 从而避免节点之间的split和merge&lt;/p>
&lt;h3 id="right-only-appends">Right-only appends
&lt;/h3>&lt;p>很多场景下 ， 数据库都有一个自增的键比如一个自增的id， 如果还是采用常规的自上而下的查找方式是纯粹的浪费时间。
Postgresql等数据库， 会直接比较插入的key的值和最右页（存储最大数据的page）的第一个key进行比较， 如果查要插入的key的值更大且该页仍有足够空间， 那么就直接把数据插入到这个最右页上， 避免了自上而下的查找（计算复杂度变化： LogN -&amp;gt; 常数时间，改进很大），这种方式也叫fast path&lt;/p>
&lt;h3 id="压缩">压缩
&lt;/h3>&lt;p>数据的压缩会牵涉到一个很重要的指标： 压缩率:&lt;/p>
&lt;ul>
&lt;li>更高的压缩率意味着更好的空间利用率&lt;/li>
&lt;li>同时意味着， 更高的时间复杂度， 因为解压缩会消耗大量的cpu资源&lt;/li>
&lt;/ul>
&lt;p>很明显， 这里有一个非常重要的tradeoff : 空间vs时间&lt;/p>
&lt;p>另外通常来讲， 我们不会再文件层级去压缩数据， 只会在page级别， 很明显前者会影响查询效率。&lt;/p>
&lt;p>另外压缩的方式除了和page绑定在一起， 也可以作为一种插件的形式进行， 和page的管理进行解耦（比如对整个column的数据进行压缩， 其实列式数据就是这么做的）&lt;/p>
&lt;h2 id="第五章-transaction-processing-and-recovery">第五章 Transaction Processing and Recovery
&lt;/h2>&lt;h3 id="buffer-managment">Buffer managment
&lt;/h3>&lt;p>类似于IO系统， 数据库也会使用类似缓存页(page cache)的概念来管理访问数据的缓存。&lt;/p>
&lt;p>&lt;img src="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/16.png"
width="706"
height="249"
srcset="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/16_hu2406060134070999495.png 480w, https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/16_hu5194839600468494596.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="283"
data-flex-basis="680px"
>&lt;br>
上图展示了B+树的节点Page在缓存池以及在磁盘上的对应关系。
Page cache机制的包括：&lt;/p>
&lt;ul>
&lt;li>将数据保留在内存&lt;/li>
&lt;li>如果要的Page在内存中， 直接返回缓存页&lt;/li>
&lt;li>如果需要的Page不再内存中， 且Buffer pool足够大， 会将该页存储在缓存池（page in）&lt;/li>
&lt;li>如果没有足够空间来存储新的缓存页， 就需要运行某种机制淘汰（eviction）一部分缓存页（比如使用LFU, LRU， CLOCK等算法）， 然后当缓存页被淘汰的时候， 会写道磁盘以保证数据的一致性&lt;/li>
&lt;/ul>
&lt;p>缓存页可以看作是数据请求和磁盘之间的中间层， 除了数据的读取， 数据的变更其实也是优先发生在这个中间层， 而不是直接写入到磁盘， 在内存中的缓存页数据被修改的时候， 这个时候这个页面就会被标识脏页(dirty page), 标识和磁盘数据不同步。
当然dirty page中的数据最后一定会被写入到磁盘， 为了保证这个过程不会出现纰漏（比如由于停电等原因， 改变的数据没有被同步到磁盘， 进而影响了数据库的ACID保证）， 通常会使用一种叫做WAL（write ahead log, 预写日志）的机制来保证数据的一致性&lt;/p>
&lt;blockquote>
&lt;p>WAL其实就是一系列的数据变更记录， 通常会在事务成功发生之后（commit）被保存在磁盘， 由于很多条记录其实可以被合并成一条， 比如对一个账户A增加1000， 记录一次， 减少1000再记录一次， 那这两条记录就可以被抵消掉， 另外WAL虽然是基于磁盘的， 但是它是顺序写入而不是随机写入 ，所以它的写入速度也是很快的&lt;/p>
&lt;/blockquote>
&lt;h3 id="缓存页淘汰机制">缓存页淘汰机制
&lt;/h3>&lt;p>常见的有LRU（Least Recently Used）, LFU(Least Freaquntly Used)，CLOCK。 LRU会比较侧重一个时效性， 越是被最近访问的Page原容易被缓存（最新访问的page会被放到链表的尾部， 然后链表头部的数据会被淘汰掉）， 在实践中(不只是数据库领域)， LFU通常会更常用一点， LFU相较于LRU会更侧重Page的访问频率， 低频的页会被有限淘汰&lt;/p>
&lt;p>Clock机制&lt;/p>
&lt;p>&lt;img src="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/17.png"
width="221"
height="226"
srcset="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/17_hu1550703330011799714.png 480w, https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/17_hu5380227924730648811.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="97"
data-flex-basis="234px"
>&lt;br>
clcok机制就是用一种环形链表将所有的Page组织在一起， 当页面被访问， 它就会被设置为1（上图灰色所示）， 当后台运行淘汰任务的时候， &amp;lsquo;指针&amp;rsquo;随机指向一个Page, 如果它是0（最近没有被访问）就会被淘汰掉， 也就是说它和LRU相比， 增加了一个随机性
如果对clock机制进行一定的改造， 把1和0的bit标识改成数值(被引用次数)， 然后当一个缓存页码，每次访问的时候， 它对应的数值就会+1， 淘汰指针转动的时候则-1, 如果是0就该页就会被淘汰， 那么它就被改造成了类似LFU的模式&lt;/p>
&lt;h3 id="数据库日志">数据库日志
&lt;/h3>&lt;p>我们在前面的Buffer management的时候提到过淘汰缓存页时，就需要将数据写入磁盘， 为了保证数据的一致性， 需要用到WAL。WAL本质上是一种日志， 在某些数据库还会按照功能区分为： redo日志， undo日志， 简单来讲redo是为了数据恢复的， undo是为了将已经保存到了数据库的数据撤回的。
然后日志的类型会有两种， 一种是物理日志(pysical log)， 日志文件将会将数据直接记录在日志中； 另一种是逻辑日志(logical log)， 逻辑日志只记录了操作而没有数据。因为物理日志有数据， 所以通常会被用在redo阶段， 这样数据恢复会更快， 逻辑日志则会被用在undo阶段
这里我们说之所以会需要undo数据（写入了磁盘的数据并没有完成commit）， 写入磁盘的操作允许被发生在commit之后， 这种数据库策略通常被称为no-force/steal 策略，相对的force/no-steal则会要求数据在commit之前写入数据， 所以force策略下， 数据库在恢复的时候只要关心redo日志就好， 因为根据策略， 但凡是日志中显示commit是先需要写入磁盘的。
但是为啥数据库还会允许no-force的这种发生写盘操作在commit之后的操作存在呢， 原因也很简单， 提升tansaction的速度。
最后， 关于数据库日志还有一个概念是checkpoint， 顾名思义就是日志的记录点， 每个checkpoint都有一个对应的LSN（log sequence number, 单调增）， 目的是为了替身redo和undo效率的， 有了checkpont我自要从上一个LSN进行数据恢复操作就好了， 而不需要每次把所有脏页进行写盘&lt;/p>
&lt;h3 id="并发控制">并发控制
&lt;/h3>&lt;p>数据库中由于并发引起的读写异常， 其实和隔离级别（isolation level）有关， 常见的隔离级别有(从低到高)：&lt;/p>
&lt;ul>
&lt;li>Read Uncomiited, 允许事务并发读取别的没有提交的事务， 这种隔离级别下， 很容易导致脏读&lt;/li>
&lt;li>Read Commited, 一个事务只能去读已经被提交了记录。但是这种隔离级别依然无法保证对相同数据同时读取（已经都是提交了的）， 能观察到相同的数据； 因为有可能在数据读取的过程中， 数据发生了改变， 导致并非读看到不一样结果&lt;/li>
&lt;li>Reapetable Read，所以在上一个隔离级别的基础上， 这个级别能保证 对数据的同时访问， 获取相同的结果&lt;/li>
&lt;li>Serializable， 最强的隔离级别， 意味着所有的事务都是按照时序展开， 会有最强的数据一致性保证，也就意味着最糟糕的并发性能&lt;/li>
&lt;/ul>
&lt;p>下图展示了各种隔离级别对应的读写异常的可能性， 可以看到Seriaizable基本是绝缘所有并发引起的读写异常， 当然代价就是牺牲并发.&lt;/p>
&lt;p>&lt;img src="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/18.png"
width="435"
height="105"
srcset="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/18_hu11750801711644344439.png 480w, https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/18_hu17448313141079290095.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="414"
data-flex-basis="994px"
>&lt;/p>
&lt;h3 id="3种并发控制流派">3种并发控制流派
&lt;/h3>&lt;p>有三种常见的并发控制流派：&lt;/p>
&lt;ul>
&lt;li>OCC， Optimistic concurrency control&lt;/li>
&lt;li>MVCC, Multiversion concurency control&lt;/li>
&lt;li>PCC, Pessimistic concurency control&lt;/li>
&lt;/ul>
&lt;h4 id="occ">OCC
&lt;/h4>&lt;p>把事务分成三个阶段：&lt;/p>
&lt;ul>
&lt;li>Read phase&lt;/li>
&lt;li>Validation phase&lt;/li>
&lt;li>Write phase&lt;/li>
&lt;/ul>
&lt;p>在第一阶段， 会把所有并发的事务的dependency写入到read set（不改变数据库状态的操作）和write set中（会有副作用， 会改变数据库操作）
第二阶段， 就是根据read set和write set中的操作， 特别是有相交（冲突）部分的操作， 对那些可能会受影响的事务， 比如一个读相关的事务， 它的数据正在被另一个事务写， 那么这个读的事务就会被终止， 不然会导致脏读
第三阶段， 当然， 如果在第二阶段中没有检查到冲突，就可以顺利的提交&lt;/p>
&lt;h4 id="mvcc">MVCC
&lt;/h4>&lt;p>简单来讲就是给所有的事务一个单调增的事务ID， MVCC通常不会阻止你去取旧的数据， 但是通常会确保在系统里面只存在一个没有提交的事务版本（version）&lt;/p>
&lt;blockquote>
&lt;p>所以这种方式应该会产生脏读， 但是因为只允许耽搁未提交事务版本， 所以最终一致性还是可以保证的&lt;/p>
&lt;/blockquote>
&lt;h4 id="pcc">PCC
&lt;/h4>&lt;p>PCC通常会和锁一起出现， 当然也有不需要要锁的实现方式， 其中最简单的PCC实现方式就是基于时间戳， 通常会有两个时间戳：&lt;/p>
&lt;ul>
&lt;li>max_read_timestamp&lt;/li>
&lt;li>max_write_timestamp&lt;/li>
&lt;/ul>
&lt;p>除了写操作被允许可以在max_write_timestamp之前被执行， 其他的操作不能在另一个读写操作的最大timestamp执行
相对而言， 基于锁的实现方式会更加流行&lt;/p>
&lt;h3 id="锁">锁
&lt;/h3>&lt;h4 id="死锁deadlock">死锁Deadlock
&lt;/h4>&lt;p>死锁会发生在两个进程或者说事务都在等待彼此去释放锁， 有点类似于循环引用的问题(A模块要导入b， b模块也要导入A)&lt;/p>
&lt;p>&lt;img src="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/19.png"
width="352"
height="194"
srcset="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/19_hu8443251899596572147.png 480w, https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/19_hu12217765076026390667.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="181"
data-flex-basis="435px"
>
怎么去解决死锁呢， 当然一种简单的方式就是给登台加上一个timeout时间， 这样就可以避免陷入无限等待。
还有方式就是让系统去识别这种可能造成死锁的状态， 也就是一个后台程序去检查事务直接存在不存在上面的waits-for graph， 如果存在就停掉其中的一个事务。&lt;/p>
&lt;h4 id="latches">Latches
&lt;/h4>&lt;p>Latch也是锁， 但是和lock不一样的点在于， lock更多的是一种更高高级的抽象概念，远离数据库内核的存储结构。 而latch， 则是更接近数据存储结构的， 它是用来解决Page层面也就是更底层的竞争问题的。
我们可你是不希望一个Page在被更高或者说要被split和merge的时候被访问， 因为这样很可能会导致我们前面提到的读异常， 所以需要Latch在更底层帮我们去控制竞争。
但是由于B树的特殊性， 我们知道子节点的merge和split很有可能会向上传播到root（当然只是可能， 并不是绝对）， 那么仅仅在某个页面上进行竞争控制是不够的。一种相对朴素的实现方法， 就是对整个访问路径上的所有Page加上Latch, 淡然这种肯定胡牺牲性能。
所以更常用的方式， 是使用Latch crabbing的一种方式：这种方式它不会一直保留整个访问路径上的page上的锁， 而是从根节点开始， 不断向下， 只要当前节点不是full的状态(意味着大概率不会被至下而上的split merge影响） 我们就打开这个锁， 所以在特点的时间节点， Latch crabing只会给很小一部分page加锁&lt;/p>
&lt;p>&lt;img src="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/20.png"
width="471"
height="515"
srcset="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/20_hu2416453500191070531.png 480w, https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/20_hu7099331628690967540.png 1024w"
loading="lazy"
alt="imgae.png"
class="gallery-image"
data-flex-grow="91"
data-flex-basis="219px"
>&lt;/p>
&lt;h2 id="第六章-b-tree-variants">第六章 B-Tree Variants
&lt;/h2>&lt;p>如何提升基于B-Tree存储结构的数据库读写效率呢， 主要的改进方向其实就是-&amp;gt;减少访问硬盘的次数和时间（特别是减少小批量的写入）, 再这种所提到的所有改进方法都是围绕这个核心点来的&lt;/p>
&lt;h3 id="copy-on-write-b-tree">Copy-on-write B-Tree
&lt;/h3>&lt;p>传统的B-树， 一般底层使用latch来解决再并发时的问题， copy-on-write直接舍弃了这种方式，直接复制会被修改的page(也就是脏页)，同时不会阻止用户访问旧数据。
当新的Page结构被构建完成的时候， 更新原有B-Tree的指针， 指向新的Page即可:&lt;/p>
&lt;p>&lt;img src="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/21.png"
width="470"
height="172"
srcset="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/21_hu17143619289894138059.png 480w, https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/21_hu6068043162215445689.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="273"
data-flex-basis="655px"
>&lt;br>
这种方式无疑是增加了磁盘空间的消耗， 好处有:&lt;/p>
&lt;ul>
&lt;li>避免了latch的使用&lt;/li>
&lt;li>读写互不影响&lt;/li>
&lt;li>数据库系统也不会处在一种被污染的状态(无非就是新的页的pointer没有替换掉老的， 但是原来的树结构和数据还是完整的， 只是过时了而已)&lt;/li>
&lt;/ul>
&lt;h3 id="lazy--b-trees">Lazy B-Trees
&lt;/h3>&lt;p>lazy B-Tree主要时通过结合内存或者说优化page cache的方式， 减少磁盘访问。以mongodb种使用的默认存储引擎WiredTiger为例（简称WT）
WT首先会保留一份磁盘种的树结构， 当然只是Index， 并没有具体的数据。当某个page首次被读取的时候，这个page的内容会复制到到对应的update buffers， 用户可以被允许直接访问update buffers中的数据（还没被写入磁盘）， 后天程序会周期性地将update buffer数据写入到磁盘取覆盖原来的Page， 如果覆盖页的size大于原来的size， 它会拆分成多个页
&lt;img src="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/22.png"
width="667"
height="419"
srcset="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/22_hu7276162576927427713.png 480w, https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/22_hu10032017794182745933.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="159"
data-flex-basis="382px"
>&lt;/p>
&lt;h3 id="fd-tree">FD-Tree
&lt;/h3>&lt;p>FD-Tree结合了B+tree和LSM树的特征（核心特点就是从随机磁盘写入-&amp;gt;顺序写入）。
它在最顶层（也称为L0）会维护一颗树， L0在无法存储更多key的时候， 会把key融合到下一层（不是所有key， 而是特殊的key， 也被称为fence, 这些fence会有一个指向下一层的Pointer
&lt;img src="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/23.png"
width="672"
height="224"
srcset="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/23_hu3766607018793258339.png 480w, https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/23_hu280856478935752144.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="300"
data-flex-basis="720px"
>&lt;br>
上图是FD-Tree的一个样例，可以看到最的顶层（L0）是一个树结构， 然后灰色的方框就是特殊的Index entry， 指向下一level的array的相同entry, 但是fence也有开那个中类型， 一种是external， 这种类型会被merge到下一层（也就是下一层会有一个相同的entry）， 另外一种是internal（比如上面的88）， 你会发现它只出现在L1没有出现在L2.
FD-Tree的查找复杂度也是Log数量级&lt;/p>
&lt;h2 id="第七章">第七章
&lt;/h2>&lt;p>前面一章， 对数据库优化方向主要围绕减少磁盘写入开销的来的， 以WiredTiger为例的方法， 都是通过数据的缓存管理来实现上面这目标， 还有一种优化的方向， 就是把耗时的磁盘随机写入操作转化为磁盘的顺序写入， LSM-Tree就是这种思想的集大成者。
LSM-Tree， 全称log structed merge， 这个名字本身就昭示了这种存储结构的特征：&lt;/p>
&lt;ul>
&lt;li>log structure： 像日志一样只会在磁盘中追加写入(append only)&lt;/li>
&lt;li>merge， 不在磁盘中删改数据， 通过merge来应对不同版本数据的问题&lt;/li>
&lt;/ul>
&lt;p>LSM树由于append only的特征， 所以这种结构是非常适合写多于读的场景（吞吐量ingestion较高的场景）， 另外由于读写在结构上互不影响， 所以不会受类似B-tree的锁机制影响并发性能&lt;/p>
&lt;h3 id="lsm的构成">LSM的构成
&lt;/h3>&lt;ul>
&lt;li>Memtable(下面两个都算是memetable, 前者接受写， 后者只接受读）， 在内存中&lt;/li>
&lt;li>Current memtable&lt;/li>
&lt;li>Fushing memtable&lt;/li>
&lt;li>On-disk flush target&lt;/li>
&lt;li>Flush tables&lt;/li>
&lt;li>Compacting tables&lt;/li>
&lt;li>Compated tabe&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/24.png"
width="600"
height="241"
srcset="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/24_hu7516303752207364248.png 480w, https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/24_hu15231139590534022483.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="248"
data-flex-basis="597px"
>&lt;br>
上图显示了LSM的整个生命周期，其中除了Current Memtable同时支持读写以外， 其他的阶段只支持读数据， 不支持数据的写入
另外有两个要点：&lt;/p>
&lt;ul>
&lt;li>数据在内存中的时候， 它是排过序的&lt;/li>
&lt;li>数据在被写入磁盘的时候， 也会用的WAL， 防止数据丢失&lt;/li>
&lt;li>当数据被写入磁盘之后， 内存中的数据就会消失， 那么后续相关数据的查询之恶能通过磁盘中的数据获取&lt;/li>
&lt;/ul>
&lt;h3 id="lsm中的更新和删除">LSM中的更新和删除
&lt;/h3>&lt;p>更新的话很简单， 只要写入新的数据就好， 新的数据会在后买你覆盖旧数据，问题在于如何删除数据。由于我们不会直接删除LSM存储在磁盘的数据， 如果一个数据同时存储在Memtable和Disk table的情况下， 我如何在不直接删除Disk table的数据来实现删除呢。 常用的方法就是为这个需要被删除的key设置一个tomestone（墓碑）， 这样在merge的时候，老的数据会被tomestone覆盖掉， 就达到了删除的效果&lt;/p>
&lt;h3 id="lsm中的merge操作">LSM中的merge操作
&lt;/h3>&lt;p>因为数据会被存储在多个磁盘区域， 很有可能多个disk table这间会有数据冲突的问题， 即它们都保有相同key的数据， 所以需要通过merge来解决冲突
整个流程划分为一下三个步骤：&lt;/p>
&lt;ul>
&lt;li>从不同的迭代器(就是存储在不同区域的磁盘数据， 他们本身是sorted的， 所以可以支持iteration)依次获取数据&lt;/li>
&lt;li>把这些候选值入队（优先队列，可以通过min heap小顶堆来实现)， 最小的那个值放入结果集&lt;/li>
&lt;li>继续从被选择的迭代器中补充数据加入队列&lt;/li>
&lt;/ul>
&lt;p>重复上面的流程， 如果第二个流程遇到key相同的情况， 可以选择通过记录本身携带的timestampe来解决冲突&lt;/p>
&lt;h3 id="lsm中的压缩compact">LSM中的压缩(compact)
&lt;/h3>&lt;p>常用的方法有Leveled compaction和Size-tiered
这里以Leveled 为例， 位于顶层(接近0)的table会将数据不断“下沉”
到下一个level（前提是当前的level放不下更多table了）， 这个过程会对key的范围重合的table进行融合&lt;/p>
&lt;p>&lt;img src="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/25.png"
width="579"
height="170"
srcset="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/25_hu10583470682730531731.png 480w, https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/25_hu13307338455303058902.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="340"
data-flex-basis="817px"
>&lt;br>
比如level 1 和 Level 2中的阴影部分， 它们key range 重合了， 所以需要融合。 融合的结果就是， 上面的数据， 也就是新的数据在观感上会一点点地“下沉”到下一个level。
另外， 通常来说下一层的Level会保留上一层2倍左右的大小&lt;/p>
&lt;h3 id="lsm的读写空间放大">LSM的读/写/空间放大
&lt;/h3>&lt;p>我们优化数据的目标其实就是解决写放大， 读放大以及空间放大的问题：&lt;/p>
&lt;ul>
&lt;li>读放大：需要读取多个地址来获取数据, LSM会有这个问题&lt;/li>
&lt;li>写放大： 需要大量的重写操作， B树有这个问题， 在涉及到page层面的数据增删的时候， LSM的compact阶段其实也有&lt;/li>
&lt;li>空间放大： LSM会有冗余数据， 所以这个很明显是有的&lt;/li>
&lt;/ul>
&lt;p>有一种指标RUM conjection（RUM分别代表read，update以及memeory）可以被用来比较及衡量一个数据库引擎的综合性能。我们需要了解的是， 通常来说， 解决上面中的任一一个问题， 都需要付出其他两个点中的1个或2个作为代价， 有点类似于分布式理论中的CAP理论， 你们很难同时拥有所有好处&lt;/p>
&lt;h3 id="lsm-实现细节">LSM 实现细节
&lt;/h3>&lt;h4 id="sstablesorted-string-table">SSTable(sorted string table)
&lt;/h4>&lt;p>LSM树中的table是基于SSTable实现的&lt;/p>
&lt;p>&lt;img src="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/26.png"
width="749"
height="339"
srcset="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/26_hu11263276291694691962.png 480w, https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/26_hu3896828638660207877.png 1024w"
loading="lazy"
alt="sst.png"
class="gallery-image"
data-flex-grow="220"
data-flex-basis="530px"
>&lt;br>
SST也会用到hash表， 但是是一个稀疏的hash表， 只记录了部分key和它们的位置（offset）， 数据本身是以key和value连续存储在磁盘上的&lt;/p>
&lt;h4 id="bloom-filter">Bloom filter
&lt;/h4>&lt;p>LSM存在的一个性能瓶颈就是读放大， 就是它的数据存储在多个table， 如果存在一个key， 在所有的数据文件中并不存在， 它依然需要取读所有的文件， 从而成为性能瓶颈， 所以可以使用Bloom filter来过滤掉不存在的key， 今儿减少读放大引起的性能问题&lt;/p>
&lt;h4 id="skiplist">Skiplist
&lt;/h4>&lt;p>将文件顺序地存储在内存的一种方式就是使用skiplist&lt;/p>
&lt;h3 id="unordered-lsm-storage">Unordered LSM Storage
&lt;/h3>&lt;p>一一般来说lsm的数据都是以有序地形式被存储的， 当然也有并不是有序形式存储的结构&lt;/p>
&lt;h4 id="bitcask">Bitcask
&lt;/h4>&lt;p>&lt;img src="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/27.png"
width="526"
height="313"
srcset="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/27_hu1752955340620232179.png 480w, https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/27_hu6532847045990745788.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="168"
data-flex-basis="403px"
>&lt;/p>
&lt;p>bitcask会在内存中保留一份key的最新路径的hash表(Keydir), keydir会在数据库初始话的时候被加载到内存， 所以很自然地会导致初始化时间过长的问题， 同时由于数据是直接被追加到磁盘的， 并不是有序（也没有什么memtable）， 所以bitcask也不支持范围查询（range query)。
但是它的优势也很明显， 首先点查询非常快， 同时写入数据是直接追加的所以写入性能也很好&lt;/p>
&lt;h4 id="wisckey">Wisckey
&lt;/h4>&lt;p>&lt;img src="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/28.png"
width="600"
height="341"
srcset="https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/28_hu11538987639246787095.png 480w, https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/imgs/28_hu15540042157964144895.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="175"
data-flex-basis="422px"
>&lt;/p>
&lt;p>wisckey会将index和数据记录分开记录（SST， key和value是存储在一起的）， 分别存储在index lsm tree和vlog files。
vlogs files类似bitcask， 是无序地， 顺序增加的日志文件index lsm tree保留了指向vLog的指针， 所以它可以保留了范围查询的优点
它的缺点是， 有序vlog没有关于数据的l生命周期的信息(liveness, 也就是数据是不是仍然是有效的)， 所以在垃圾回收的时候， 必须要遍历左侧的index tree， 增加了复杂度。 传统的lsm， 哪怕是被删除的数据， 也可以在数据压缩阶段被直接覆盖(前面提到的类似墓碑的标记)&lt;/p></description></item></channel></rss>