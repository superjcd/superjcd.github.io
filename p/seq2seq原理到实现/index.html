<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="Seq2Seq 模型架构 seq2seq是2014年随论文[Sequence to Sequence Learning with Neural Networks]发布 的一种NLG自然语言（生成式）模型， 下图直观展示了seq2seq模型在机器翻译任务（德译英）中，神经网络前向计算（Forwad Computiation）的工作流程\nseq2seq模型的关键点在于， 将模型分成了编码器和解码器：\n编码器在编码阶段，会对每一个Token（上面的话是一个个的单词）进行编码，编码过程通常会使用RNN来实现(论文里使用的是LSTM， 所以会同时有hidden state和cell两个输出， 如果是最原始的RNN， 那么应该就只有一个hidden state)。\n解码器会沿用编码器的结构， 比如这里的编码器用的是双层的LSTM(论文里面是4层)， 那么解码器也会使用双层的LSTM， 同时解码器的最初的hidden state和cell刚好就是解码器的最后一步产出的hidden state和cell\nhidden state的维度和cell的维度通常会一样\n编码器 我们拆解一下整个seq2seq， 先只关注编码器这个部分\n编码器前向计算涉及的公式：\ne指的是embedding词嵌入\n也就是当我们使用一般的RNN模型时， 就是上面的公式， 如果是LSTM, 因为会涉及cell（记忆单元）所以会多一个参数。上面的公式没有涉及层级， 如果涉及多个层级（只有第一层会用到embedding）， 以LSTM模型为例， 公式为：\n从第二层layer开始， 编码器的第一个参数变成了前一层的hidden state而不是embbeding\n编码器pytorch实现 1 2 3 4 5 6 7 8 9 10 11 12 13 class Encoder(nn.Module): def __init__(self, input_dim, embedding_dim, hidden_dim, n_layers, dropout): super().__init__() self.hidden_dim = hidden_dim self.n_layers = n_layers self.embedding = nn.Embedding(input_dim, embedding_dim) self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout) self.dropout = nn.Dropout(dropout) def forward(self, src): embedded = self.dropout(self.embedding(src)) outputs, (hidden, cell) = self.rnn(embedded) return hidden, cell 这里注意负责前向计算逻辑的forward函数中参数src的形状是：输入句子长度（或者说token数） * Bacth大小，因为LSTM会默认接受输入句子长度 * Bacth大小 * Embedding大小的输入， 虽然可以将Batch大小放到第一维度， 但是考虑到seq2seq其实是根据一个一个的token进行前向运算的运算的， 所以第一个维度是句子长度是更符合逻辑的， 相当于将token置于for loop的最外层（虽然可能不符合多数人的直觉）\n"><title>Seq2Seq原理到实现</title>
<link rel=canonical href=https://superjcd.github.io/p/seq2seq%E5%8E%9F%E7%90%86%E5%88%B0%E5%AE%9E%E7%8E%B0/><link rel=stylesheet href=/scss/style.min.0304c6baf04e01a8fe70693791cb744d56a3578a3120a8796cefc66825aa39c7.css><meta property='og:title' content="Seq2Seq原理到实现"><meta property='og:description' content="Seq2Seq 模型架构 seq2seq是2014年随论文[Sequence to Sequence Learning with Neural Networks]发布 的一种NLG自然语言（生成式）模型， 下图直观展示了seq2seq模型在机器翻译任务（德译英）中，神经网络前向计算（Forwad Computiation）的工作流程\nseq2seq模型的关键点在于， 将模型分成了编码器和解码器：\n编码器在编码阶段，会对每一个Token（上面的话是一个个的单词）进行编码，编码过程通常会使用RNN来实现(论文里使用的是LSTM， 所以会同时有hidden state和cell两个输出， 如果是最原始的RNN， 那么应该就只有一个hidden state)。\n解码器会沿用编码器的结构， 比如这里的编码器用的是双层的LSTM(论文里面是4层)， 那么解码器也会使用双层的LSTM， 同时解码器的最初的hidden state和cell刚好就是解码器的最后一步产出的hidden state和cell\nhidden state的维度和cell的维度通常会一样\n编码器 我们拆解一下整个seq2seq， 先只关注编码器这个部分\n编码器前向计算涉及的公式：\ne指的是embedding词嵌入\n也就是当我们使用一般的RNN模型时， 就是上面的公式， 如果是LSTM, 因为会涉及cell（记忆单元）所以会多一个参数。上面的公式没有涉及层级， 如果涉及多个层级（只有第一层会用到embedding）， 以LSTM模型为例， 公式为：\n从第二层layer开始， 编码器的第一个参数变成了前一层的hidden state而不是embbeding\n编码器pytorch实现 1 2 3 4 5 6 7 8 9 10 11 12 13 class Encoder(nn.Module): def __init__(self, input_dim, embedding_dim, hidden_dim, n_layers, dropout): super().__init__() self.hidden_dim = hidden_dim self.n_layers = n_layers self.embedding = nn.Embedding(input_dim, embedding_dim) self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout) self.dropout = nn.Dropout(dropout) def forward(self, src): embedded = self.dropout(self.embedding(src)) outputs, (hidden, cell) = self.rnn(embedded) return hidden, cell 这里注意负责前向计算逻辑的forward函数中参数src的形状是：输入句子长度（或者说token数） * Bacth大小，因为LSTM会默认接受输入句子长度 * Bacth大小 * Embedding大小的输入， 虽然可以将Batch大小放到第一维度， 但是考虑到seq2seq其实是根据一个一个的token进行前向运算的运算的， 所以第一个维度是句子长度是更符合逻辑的， 相当于将token置于for loop的最外层（虽然可能不符合多数人的直觉）\n"><meta property='og:url' content='https://superjcd.github.io/p/seq2seq%E5%8E%9F%E7%90%86%E5%88%B0%E5%AE%9E%E7%8E%B0/'><meta property='og:site_name' content='superjcd'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='seq2seq'><meta property='article:published_time' content='2024-01-18T00:00:00+00:00'><meta property='article:modified_time' content='2024-01-18T00:00:00+00:00'><meta name=twitter:title content="Seq2Seq原理到实现"><meta name=twitter:description content="Seq2Seq 模型架构 seq2seq是2014年随论文[Sequence to Sequence Learning with Neural Networks]发布 的一种NLG自然语言（生成式）模型， 下图直观展示了seq2seq模型在机器翻译任务（德译英）中，神经网络前向计算（Forwad Computiation）的工作流程\nseq2seq模型的关键点在于， 将模型分成了编码器和解码器：\n编码器在编码阶段，会对每一个Token（上面的话是一个个的单词）进行编码，编码过程通常会使用RNN来实现(论文里使用的是LSTM， 所以会同时有hidden state和cell两个输出， 如果是最原始的RNN， 那么应该就只有一个hidden state)。\n解码器会沿用编码器的结构， 比如这里的编码器用的是双层的LSTM(论文里面是4层)， 那么解码器也会使用双层的LSTM， 同时解码器的最初的hidden state和cell刚好就是解码器的最后一步产出的hidden state和cell\nhidden state的维度和cell的维度通常会一样\n编码器 我们拆解一下整个seq2seq， 先只关注编码器这个部分\n编码器前向计算涉及的公式：\ne指的是embedding词嵌入\n也就是当我们使用一般的RNN模型时， 就是上面的公式， 如果是LSTM, 因为会涉及cell（记忆单元）所以会多一个参数。上面的公式没有涉及层级， 如果涉及多个层级（只有第一层会用到embedding）， 以LSTM模型为例， 公式为：\n从第二层layer开始， 编码器的第一个参数变成了前一层的hidden state而不是embbeding\n编码器pytorch实现 1 2 3 4 5 6 7 8 9 10 11 12 13 class Encoder(nn.Module): def __init__(self, input_dim, embedding_dim, hidden_dim, n_layers, dropout): super().__init__() self.hidden_dim = hidden_dim self.n_layers = n_layers self.embedding = nn.Embedding(input_dim, embedding_dim) self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout) self.dropout = nn.Dropout(dropout) def forward(self, src): embedded = self.dropout(self.embedding(src)) outputs, (hidden, cell) = self.rnn(embedded) return hidden, cell 这里注意负责前向计算逻辑的forward函数中参数src的形状是：输入句子长度（或者说token数） * Bacth大小，因为LSTM会默认接受输入句子长度 * Bacth大小 * Embedding大小的输入， 虽然可以将Batch大小放到第一维度， 但是考虑到seq2seq其实是根据一个一个的token进行前向运算的运算的， 所以第一个维度是句子长度是更符合逻辑的， 相当于将token置于for loop的最外层（虽然可能不符合多数人的直觉）\n"><link rel="shortcut icon" href=/favicon.ico></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu12125167938935615648.png width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>🍥</span></figure><div class=site-meta><h1 class=site-name><a href=/>superjcd</a></h1><h2 class=site-description></h2></div></header><ol class=menu-social><li><a href=https://github.com/superjcd target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/links/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>Links</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#seq2seq>Seq2Seq</a><ol><li><a href=#模型架构>模型架构</a></li><li><a href=#编码器>编码器</a></li><li><a href=#编码器pytorch实现>编码器pytorch实现</a></li><li><a href=#解码器>解码器</a></li><li><a href=#解码器pytorch实现>解码器pytorch实现</a></li><li><a href=#完整seq2seq代码实现>完整seq2seq代码实现</a></li></ol></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/categories/nlp/>NLP</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/seq2seq%E5%8E%9F%E7%90%86%E5%88%B0%E5%AE%9E%E7%8E%B0/>Seq2Seq原理到实现</a></h2></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Jan 18, 2024</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>2 minute read</time></div></footer></div></header><section class=article-content><h2 id=seq2seq>Seq2Seq</h2><h3 id=模型架构>模型架构</h3><p>seq2seq是2014年随论文[<a class=link href=https://arxiv.org/abs/1409.3215 target=_blank rel=noopener>Sequence to Sequence Learning with Neural Networks</a>]发布
的一种NLG自然语言（生成式）模型， 下图直观展示了seq2seq模型在机器翻译任务（德译英）中，神经网络前向计算（Forwad Computiation）的工作流程</p><p><img src=/p/seq2seq%E5%8E%9F%E7%90%86%E5%88%B0%E5%AE%9E%E7%8E%B0/imgs/seq2seq.png width=959 height=599 srcset="/p/seq2seq%E5%8E%9F%E7%90%86%E5%88%B0%E5%AE%9E%E7%8E%B0/imgs/seq2seq_hu1264161868197847433.png 480w, /p/seq2seq%E5%8E%9F%E7%90%86%E5%88%B0%E5%AE%9E%E7%8E%B0/imgs/seq2seq_hu12533742928908207377.png 1024w" loading=lazy alt=seq2seq class=gallery-image data-flex-grow=160 data-flex-basis=384px><br>seq2seq模型的关键点在于， 将模型分成了编码器和解码器：<br>编码器在编码阶段，会对每一个Token（上面的话是一个个的单词）进行<strong>编码</strong>，编码过程通常会使用RNN来实现(论文里使用的是LSTM， 所以会同时有hidden state和cell两个输出， 如果是最原始的RNN， 那么应该就只有一个hidden state)。<br>解码器会沿用编码器的结构， 比如这里的编码器用的是双层的LSTM(论文里面是4层)， 那么解码器也会使用双层的LSTM， 同时解码器的最初的hidden state和cell刚好就是解码器的最后一步产出的hidden state和cell</p><blockquote><p>hidden state的维度和cell的维度通常会一样</p></blockquote><h3 id=编码器>编码器</h3><p>我们拆解一下整个seq2seq， 先只关注编码器这个部分</p><p><img src=/p/seq2seq%E5%8E%9F%E7%90%86%E5%88%B0%E5%AE%9E%E7%8E%B0/imgs/encoder.png width=440 height=323 srcset="/p/seq2seq%E5%8E%9F%E7%90%86%E5%88%B0%E5%AE%9E%E7%8E%B0/imgs/encoder_hu8009643578920737105.png 480w, /p/seq2seq%E5%8E%9F%E7%90%86%E5%88%B0%E5%AE%9E%E7%8E%B0/imgs/encoder_hu13681366792852420493.png 1024w" loading=lazy alt=encoder class=gallery-image data-flex-grow=136 data-flex-basis=326px></p><p>编码器前向计算涉及的公式：</p><p><img src=/p/seq2seq%E5%8E%9F%E7%90%86%E5%88%B0%E5%AE%9E%E7%8E%B0/imgs/encoder_formula1.png width=285 height=64 srcset="/p/seq2seq%E5%8E%9F%E7%90%86%E5%88%B0%E5%AE%9E%E7%8E%B0/imgs/encoder_formula1_hu6172150119980973305.png 480w, /p/seq2seq%E5%8E%9F%E7%90%86%E5%88%B0%E5%AE%9E%E7%8E%B0/imgs/encoder_formula1_hu15051125157335467098.png 1024w" loading=lazy class=gallery-image data-flex-grow=445 data-flex-basis=1068px></p><blockquote><p>e指的是embedding词嵌入</p></blockquote><p>也就是当我们使用一般的RNN模型时， 就是上面的公式， 如果是LSTM, 因为会涉及cell（记忆单元）所以会多一个参数。上面的公式没有涉及层级， 如果涉及多个层级（只有第一层会用到embedding）， 以LSTM模型为例， 公式为：</p><p><img src=/p/seq2seq%E5%8E%9F%E7%90%86%E5%88%B0%E5%AE%9E%E7%8E%B0/imgs/encoder_formula2.png width=377 height=71 srcset="/p/seq2seq%E5%8E%9F%E7%90%86%E5%88%B0%E5%AE%9E%E7%8E%B0/imgs/encoder_formula2_hu12339779176501503145.png 480w, /p/seq2seq%E5%8E%9F%E7%90%86%E5%88%B0%E5%AE%9E%E7%8E%B0/imgs/encoder_formula2_hu5336244642429052342.png 1024w" loading=lazy class=gallery-image data-flex-grow=530 data-flex-basis=1274px></p><p>从第二层layer开始， 编码器的第一个参数变成了前一层的hidden state而不是embbeding</p><h3 id=编码器pytorch实现>编码器pytorch实现</h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>Encoder</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>input_dim</span><span class=p>,</span> <span class=n>embedding_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>,</span> <span class=n>n_layers</span><span class=p>,</span> <span class=n>dropout</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>hidden_dim</span> <span class=o>=</span> <span class=n>hidden_dim</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>n_layers</span> <span class=o>=</span> <span class=n>n_layers</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>embedding</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=n>embedding_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>rnn</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LSTM</span><span class=p>(</span><span class=n>embedding_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>,</span> <span class=n>n_layers</span><span class=p>,</span> <span class=n>dropout</span><span class=o>=</span><span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>src</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>embedded</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>embedding</span><span class=p>(</span><span class=n>src</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>outputs</span><span class=p>,</span> <span class=p>(</span><span class=n>hidden</span><span class=p>,</span> <span class=n>cell</span><span class=p>)</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>rnn</span><span class=p>(</span><span class=n>embedded</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>hidden</span><span class=p>,</span> <span class=n>cell</span>
</span></span></code></pre></td></tr></table></div></div><blockquote><p>这里注意负责前向计算逻辑的forward函数中参数src的形状是：输入句子长度（或者说token数） * Bacth大小，因为LSTM会默认接受输入句子长度 * Bacth大小 * Embedding大小的输入， 虽然可以将Batch大小放到第一维度， 但是考虑到seq2seq其实是根据一个一个的token进行前向运算的运算的， 所以第一个维度是句子长度是更符合逻辑的， 相当于将token置于for loop的最外层（虽然可能不符合多数人的直觉）</p></blockquote><p>Ok， 由于我们的Decoder只需要hidden state和cell， 所以forwar只输出hidden和cell</p><h3 id=解码器>解码器</h3><p>回过头来我们来关注解码器：<br><img src=/p/seq2seq%E5%8E%9F%E7%90%86%E5%88%B0%E5%AE%9E%E7%8E%B0/imgs/decoder.png width=285 height=465 srcset="/p/seq2seq%E5%8E%9F%E7%90%86%E5%88%B0%E5%AE%9E%E7%8E%B0/imgs/decoder_hu10388762060673316159.png 480w, /p/seq2seq%E5%8E%9F%E7%90%86%E5%88%B0%E5%AE%9E%E7%8E%B0/imgs/decoder_hu5512932341687065720.png 1024w" loading=lazy alt=decoder class=gallery-image data-flex-grow=61 data-flex-basis=147px><br>其实解码器和编码器在结构上是非常相似的， 一个比较直观的区别在于： 解码器会在每个计算步骤产出的output之上构建一个全连接层， 然后输出的大小是词汇表的大小</p><p>解码器的计算公式：<br><img src=/p/seq2seq%E5%8E%9F%E7%90%86%E5%88%B0%E5%AE%9E%E7%8E%B0/imgs/decoder_formula.png width=371 height=65 srcset="/p/seq2seq%E5%8E%9F%E7%90%86%E5%88%B0%E5%AE%9E%E7%8E%B0/imgs/decoder_formula_hu17885942291783724144.png 480w, /p/seq2seq%E5%8E%9F%E7%90%86%E5%88%B0%E5%AE%9E%E7%8E%B0/imgs/decoder_formula_hu10817745205262158215.png 1024w" loading=lazy class=gallery-image data-flex-grow=570 data-flex-basis=1369px></p><blockquote><p>这里的d其实就是目标输出的embedding layer, 类比编码器公式中的e； s就是hidden state类比编码器公式中的h</p></blockquote><h3 id=解码器pytorch实现>解码器pytorch实现</h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>Decoder</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>,</span> <span class=n>embedding_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>,</span> <span class=n>n_layers</span><span class=p>,</span> <span class=n>dropout</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>output_dim</span> <span class=o>=</span> <span class=n>output_dim</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>hidden_dim</span> <span class=o>=</span> <span class=n>hidden_dim</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>n_layers</span> <span class=o>=</span> <span class=n>n_layers</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>embedding</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>output_dim</span><span class=p>,</span> <span class=n>embedding_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>rnn</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LSTM</span><span class=p>(</span><span class=n>embedding_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>,</span> <span class=n>n_layers</span><span class=p>,</span> <span class=n>dropout</span><span class=o>=</span><span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>fc_out</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=nb>input</span><span class=p>,</span> <span class=n>hidden</span><span class=p>,</span> <span class=n>cell</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>input</span> <span class=o>=</span> <span class=nb>input</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>embedded</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>embedding</span><span class=p>(</span><span class=nb>input</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>output</span><span class=p>,</span> <span class=p>(</span><span class=n>hidden</span><span class=p>,</span> <span class=n>cell</span><span class=p>)</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>rnn</span><span class=p>(</span><span class=n>embedded</span><span class=p>,</span> <span class=p>(</span><span class=n>hidden</span><span class=p>,</span> <span class=n>cell</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>prediction</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc_out</span><span class=p>(</span><span class=n>output</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(</span><span class=mi>0</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>prediction</span><span class=p>,</span> <span class=n>hidden</span><span class=p>,</span> <span class=n>cell</span>
</span></span></code></pre></td></tr></table></div></div><p>在代码上， 解码器也是和编码器相似的（甚至rnn模型的参数也是一样的）， 当然也一些不同的地方：</p><ul><li>forward的参数多了hidden和cell， 这个是encoder的输出</li><li>input由于是一个个的token， 所以使用<code>input = input.unsqueeze(0)</code>补充了第一维度， 这是为了满足LSTM模型的输入需求</li><li>prediction预测是构建在output之上的， 由于是第一维可有可无， 我们这里用squeeze压缩掉</li></ul><h3 id=完整seq2seq代码实现>完整seq2seq代码实现</h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>Seq2Seq</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>encoder</span><span class=p>,</span> <span class=n>decoder</span><span class=p>,</span> <span class=n>device</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>encoder</span> <span class=o>=</span> <span class=n>encoder</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>decoder</span> <span class=o>=</span> <span class=n>decoder</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>device</span> <span class=o>=</span> <span class=n>device</span>
</span></span><span class=line><span class=cl>        <span class=k>assert</span> <span class=n>encoder</span><span class=o>.</span><span class=n>hidden_dim</span> <span class=o>==</span> <span class=n>decoder</span><span class=o>.</span><span class=n>hidden_dim</span>
</span></span><span class=line><span class=cl>        <span class=k>assert</span> <span class=n>encoder</span><span class=o>.</span><span class=n>n_layers</span> <span class=o>==</span> <span class=n>decoder</span><span class=o>.</span><span class=n>n_layers</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>src</span><span class=p>,</span> <span class=n>trg</span><span class=p>,</span> <span class=n>teacher_forcing_ratio</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>batch_size</span> <span class=o>=</span> <span class=n>trg</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>trg_length</span> <span class=o>=</span> <span class=n>trg</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>trg_vocab_size</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>decoder</span><span class=o>.</span><span class=n>output_dim</span>
</span></span><span class=line><span class=cl>        <span class=n>outputs</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>trg_length</span><span class=p>,</span> <span class=n>batch_size</span><span class=p>,</span> <span class=n>trg_vocab_size</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>hidden</span><span class=p>,</span> <span class=n>cell</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>encoder</span><span class=p>(</span><span class=n>src</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=nb>input</span> <span class=o>=</span> <span class=n>trg</span><span class=p>[</span><span class=mi>0</span><span class=p>,:]</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>t</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>trg_length</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>output</span><span class=p>,</span> <span class=n>hidden</span><span class=p>,</span> <span class=n>cell</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>decoder</span><span class=p>(</span><span class=nb>input</span><span class=p>,</span> <span class=n>hidden</span><span class=p>,</span> <span class=n>cell</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>outputs</span><span class=p>[</span><span class=n>t</span><span class=p>]</span> <span class=o>=</span> <span class=n>output</span>
</span></span><span class=line><span class=cl>            <span class=n>teacher_force</span> <span class=o>=</span> <span class=n>random</span><span class=o>.</span><span class=n>random</span><span class=p>()</span> <span class=o>&lt;</span> <span class=n>teacher_forcing_ratio</span>
</span></span><span class=line><span class=cl>            <span class=n>top1</span> <span class=o>=</span> <span class=n>output</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=nb>input</span> <span class=o>=</span> <span class=n>trg</span><span class=p>[</span><span class=n>t</span><span class=p>]</span> <span class=k>if</span> <span class=n>teacher_force</span> <span class=k>else</span> <span class=n>top1</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>outputs</span>
</span></span></code></pre></td></tr></table></div></div><p>这里先解释一下上面的几个新变量：</p><ul><li>trg其实是target， 表示目标输出， 所以在结构上和input是相似的: 目标字符串长度(token数) * Batch大小</li><li>teacher_forcing_ratio， 其实就是一个0到1的数值（表示概率）， 这个值表示使用上一步的真实值（ground truth）或者预测值作为下一步输入的概率， 显然易见这个值如果太大的话可能会导致过拟合</li></ul><p>当我们把encoder和decoder的逻辑放在一起的时候， 我们能清晰地看到forwad中有一个for循环会沿着输出长度一步一步调用decoder， 然后将每一步的output填充到最后的结果outputs中；</p><blockquote><p>注意， encoder在整个前向计算中只进行了一次！</p></blockquote></section><footer class=article-footer><section class=article-tags><a href=/tags/seq2seq/>Seq2seq</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer></article><div id=gitalk-container></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.css><script src=https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js></script><script src=https://cdn.jsdelivr.net/npm/blueimp-md5@2.18.0/js/md5.min.js></script><script>const gitalk=new Gitalk({clientID:"4cde8442733ac793019a",clientSecret:"06b853952b0075feb92709fa5e11392338d176d9",repo:"comments",owner:"superjcd",admin:["superjcd"],distractionFreeMode:!1,id:md5(location.pathname)});(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("gitalk-container").innerHTML="Gitalk comments not available by default when the website is previewed locally.";return}gitalk.render("gitalk-container")})()</script><footer class=site-footer><section class=copyright>&copy;
2020 -
2025 superjcd</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.26.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>