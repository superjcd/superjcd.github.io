[{"content":"Compound pattern 这个模式的它能解决怎样的问题? 其实没有解决什么问题，只是把需要共用相同state的组件用一种合理的方式组合在一起罢了。\n我们先看一下使用了Compound模式的组件代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import React from \u0026#34;react\u0026#34;; import { FlyOut } from \u0026#34;./FlyOut\u0026#34;; export default function FlyoutMenu() { return ( \u0026lt;FlyOut\u0026gt; \u0026lt;FlyOut.Toggle /\u0026gt; \u0026lt;FlyOut.List\u0026gt; \u0026lt;FlyOut.Item\u0026gt;Edit\u0026lt;/FlyOut.Item\u0026gt; \u0026lt;FlyOut.Item\u0026gt;Delete\u0026lt;/FlyOut.Item\u0026gt; \u0026lt;/FlyOut.List\u0026gt; \u0026lt;/FlyOut\u0026gt; ); } 这个模式出现在很多地方，比如Shadcn中的组件，也大多是基于多个更小的组件组合起来的。 不一定非得使用Flyout.xxxx来表示子组件, 但是这种命名方式更加清晰\nToggle，List， Item组件都复用了Flyout的状态value。\nFlyout的定义：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 const FlyOutContext = createContext(); function Flyout({children}) { const [value, setValue] = useState(false) \u0026lt;FlyOutContext.Provider value={{value, setValue}}\u0026gt; {chirdren} \u0026lt;/FlyOutContext.Provider\u0026gt; } 然后以Toggle组件为例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 Flyout.Toggle = ({chidren}) =\u0026gt; { const {value, setValue} = useContext(FlyOutContext) return ( \u0026lt;div onClick={() =\u0026gt; setVale(!value)}\u0026gt; \u0026lt;Icon /\u0026gt; \u0026lt;/div\u0026gt; ) } 其他的组件也是用一同样的方式获取到value， 这个不赘述了；这里需要追加的一点的是， 给多个子元素传递状态的方式除了上面这种使用Context， 也可以直接通过遍历React.Children结合React.cloneElement函数来实现, 比如：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 export function FlyOut(chilren) { const [value, setValue] = React.useState(false); return ( \u0026lt;div\u0026gt; {React.Children.map(children, child =\u0026gt; React.cloneElement(child, { value, setValue }) )} \u0026lt;/div\u0026gt; ); } HOC （High-order component） 基础示例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 import React from \u0026#39;react\u0026#39;; const withAsync = (WrappedComponent, LoadingComponent = () =\u0026gt; \u0026lt;div\u0026gt;Loading...\u0026lt;/div\u0026gt;) =\u0026gt; { return (props) =\u0026gt; { if (props.loading) { return \u0026lt;LoadingComponent {...props} /\u0026gt;; } return \u0026lt;WrappedComponent {...props} /\u0026gt;; }; }; export default withAsync; 上面展示了一个基础的HOC示例，withAsny函数会根据props的loading属性来决定是不是要添加一个等待的过渡效果, 好处在于逻辑复用(类似于装饰器函数)\n带选项options 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 import React from \u0026#39;react\u0026#39;; const withAsync = (WrappedComponent, options = {}) =\u0026gt; { const { LoadingComponent = () =\u0026gt; \u0026lt;div\u0026gt;Loading...\u0026lt;/div\u0026gt;, ErrorComponent = ({ error }) =\u0026gt; \u0026lt;div\u0026gt;Error: {error}\u0026lt;/div\u0026gt;, loadingPropName = \u0026#39;loading\u0026#39;, errorPropName = \u0026#39;error\u0026#39; } = options; return (props) =\u0026gt; { if (props[errorPropName]) { return \u0026lt;ErrorComponent error={props[errorPropName]} {...props} /\u0026gt;; } if (props[loadingPropName]) { return \u0026lt;LoadingComponent {...props} /\u0026gt;; } return \u0026lt;WrappedComponent {...props} /\u0026gt;; }; }; export default withAsync; 带options的HOC， 可以类比为带参数的装饰器函数, 可以更加灵活地处理不同的应用场景\n使用场景：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 function DataDisplay({ data }) { return \u0026lt;div\u0026gt;{JSON.stringify(data, null, 2)}\u0026lt;/div\u0026gt;; } const EnhancedDataDisplay = withAsync(DataDisplay, { LoadingComponent: Spinner, ErrorComponent: ErrorMessage, loadingPropName: \u0026#39;isFetching\u0026#39;, errorPropName: \u0026#39;fetchError\u0026#39; }); ... return ( \u0026lt;div\u0026gt; \u0026lt;h1\u0026gt;Data Display\u0026lt;/h1\u0026gt; \u0026lt;EnhancedDataDisplay data={state.data} isFetching={state.isFetching} fetchError={state.fetchError} /\u0026gt; \u0026lt;/div\u0026gt; ); typescipt 版本：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 import React from \u0026#39;react\u0026#39;; interface WithAsyncOptions\u0026lt;P = any\u0026gt; { LoadingComponent?: React.ComponentType\u0026lt;P\u0026gt;; ErrorComponent?: React.ComponentType\u0026lt;P \u0026amp; { error: any }\u0026gt;; loadingPropName?: string; errorPropName?: string; } function withAsync\u0026lt;P extends object\u0026gt;( WrappedComponent: React.ComponentType\u0026lt;P\u0026gt;, options: WithAsyncOptions\u0026lt;P\u0026gt; = {} ): React.FC\u0026lt;P\u0026gt; { const { LoadingComponent = () =\u0026gt; \u0026lt;div\u0026gt;Loading...\u0026lt;/div\u0026gt;, ErrorComponent = ({ error }) =\u0026gt; \u0026lt;div\u0026gt;Error: {error}\u0026lt;/div\u0026gt;, loadingPropName = \u0026#39;loading\u0026#39;, errorPropName = \u0026#39;error\u0026#39; } = options; return (props: P \u0026amp; { [key: string]: any }) =\u0026gt; { if (props[errorPropName]) { return \u0026lt;ErrorComponent error={props[errorPropName]} {...props} /\u0026gt;; } if (props[loadingPropName]) { return \u0026lt;LoadingComponent {...props} /\u0026gt;; } return \u0026lt;WrappedComponent {...props} /\u0026gt;; }; } export default withAsync; 组合多个HOC情况 compose函数(用来结合多个HOC):\n1 2 3 4 5 6 7 8 9 const compose = (...hocs) =\u0026gt; (Component) =\u0026gt; hocs.reduceRight( (WrappedComponent, hoc) =\u0026gt; hoc(WrappedComponent), Component ); 使用compose场景(结合context)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 const withUserContext = (WrappedComponent) =\u0026gt; { return (props) =\u0026gt; ( \u0026lt;UserContext.Consumer\u0026gt; {(user) =\u0026gt; \u0026lt;WrappedComponent {...props} user={user} /\u0026gt;} \u0026lt;/UserContext.Consumer\u0026gt; ); }; const EnhancedComponent = compose( withTheme, withLoading, withUserContext )(MyComponent); Render props替代HOC场景 譬如实现基于登录状态的条件渲染：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 function Auth({ children }) { const [user, setUser] = useState(null); useEffect(() =\u0026gt; { checkAuth().then(setUser); }, []); return children({ isAuthenticated: !!user, user }); } // 使用 \u0026lt;Auth\u0026gt; {({ isAuthenticated, user }) =\u0026gt; isAuthenticated ? \u0026lt;Dashboard user={user} /\u0026gt; : \u0026lt;Login /\u0026gt; } \u0026lt;/Auth\u0026gt; 但是这里还是需要注意的一点在于： 和前面的HOC的条件渲染不同， 这里的条件渲染事实上是在子组件实现的(前面的HOC是在Wrapper函数实现的)， 外部的Auth函数只是提供给Children必要的props而已； 但是可不可以把UI逻辑放到Auth组件里？其实也是可以的；但是render props模式一般只是提供给children共享的状态(比如上面的user)； 另外假设有多个子组件要共用状态的话， 可以考虑：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 \u0026lt;Auth\u0026gt; {({ isAuthenticated, user }) =\u0026gt; ( \u0026lt;\u0026gt; {isAuthenticated ? ( \u0026lt;\u0026gt; \u0026lt;Dashboard user={user} /\u0026gt; \u0026lt;Profile user={user} /\u0026gt; \u0026lt;/\u0026gt; ) : ( \u0026lt;\u0026gt; \u0026lt;Login /\u0026gt; \u0026lt;Signup /\u0026gt; \u0026lt;/\u0026gt; )} \u0026lt;/\u0026gt; )} \u0026lt;/Auth\u0026gt; 或者直接使用Context(诸如前面的Compund模式)， context适合更加复杂（比如深度嵌套）的状态共享\nProps Collection 模式 基础用法 鼠标交互逻辑\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 function useMouse() { const [position, setPosition] = useState({ x: 0, y: 0 }); const mouseProps = { onMouseMove: (e) =\u0026gt; setPosition({ x: e.clientX, y: e.clientY }), style: { cursor: \u0026#39;pointer\u0026#39; }, }; return { position, mouseProps, }; } function MouseTracker() { const { position, mouseProps } = useMouse(); return ( \u0026lt;div {...mouseProps}\u0026gt; Mouse position: {position.x}, {position.y} \u0026lt;/div\u0026gt; ); } 简单来讲Props collection就是把相关的props集合在了一起， 比如上面拿到useMouse, 把位置相关的， 以及和鼠标位置相关联的action组合在了一起\n结合Render Props模式 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 function Tooltip({ children, content }) { const [isVisible, setIsVisible] = useState(false); const tooltipProps = { onMouseEnter: () =\u0026gt; setIsVisible(true), onMouseLeave: () =\u0026gt; setIsVisible(false), }; return children({ isVisible, tooltipProps, }); } function App() { return ( \u0026lt;Tooltip content=\u0026#34;This is a tooltip\u0026#34;\u0026gt; {({ isVisible, tooltipProps }) =\u0026gt; ( \u0026lt;div\u0026gt; \u0026lt;button {...tooltipProps}\u0026gt;Hover me\u0026lt;/button\u0026gt; {isVisible \u0026amp;\u0026amp; \u0026lt;div className=\u0026#34;tooltip\u0026#34;\u0026gt;Tooltip content\u0026lt;/div\u0026gt;} \u0026lt;/div\u0026gt; )} \u0026lt;/Tooltip\u0026gt; ); } 使用多种Props集合 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 function useHover() { const [isHovered, setIsHovered] = useState(false); const hoverProps = { onMouseEnter: () =\u0026gt; setIsHovered(true), onMouseLeave: () =\u0026gt; setIsHovered(false), }; return { isHovered, hoverProps, }; } function useFocus() { const [isFocused, setIsFocused] = useState(false); const focusProps = { onFocus: () =\u0026gt; setIsFocused(true), onBlur: () =\u0026gt; setIsFocused(false), }; return { isFocused, focusProps, }; } function Button() { const { isHovered, hoverProps } = useHover(); const { isFocused, focusProps } = useFocus(); return ( \u0026lt;button {...hoverProps} {...focusProps} style={{ backgroundColor: isHovered ? \u0026#39;lightblue\u0026#39; : \u0026#39;white\u0026#39;, borderColor: isFocused ? \u0026#39;blue\u0026#39; : \u0026#39;gray\u0026#39;, }} \u0026gt; Interactive Button \u0026lt;/button\u0026gt; ); } Props Getter 模式 不直接返回Props对象， 而是说利用闭包返回可以生成Props的函数， 这样在后续使用中可以更加灵活\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 function useToggle(initialOn = false) { const [on, setOn] = useState(initialOn); const toggle = () =\u0026gt; setOn(!on); // Props Getter 函数 const getTogglerProps = ({ onClick, ...props } = {}) =\u0026gt; ({ \u0026#39;aria-pressed\u0026#39;: on, onClick: () =\u0026gt; { toggle(); onClick?.(); }, ...props, }); return { on, toggle, getTogglerProps, }; } function App() { const { on, getTogglerProps } = useToggle(); return ( \u0026lt;div\u0026gt; \u0026lt;button {...getTogglerProps({ onClick: () =\u0026gt; console.log(\u0026#39;Button clicked\u0026#39;), className: \u0026#39;my-button\u0026#39;, })} \u0026gt; {on ? \u0026#39;ON\u0026#39; : \u0026#39;OFF\u0026#39;} \u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; ); } State Reducer模式 State Reducer 模式的核心思想是：​​组件仍然管理自己的状态，但将状态更新的决定权通过 reducer 函数暴露给使用者​​\n传统Reducer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 function toggleReducer(state, action) { switch (action.type) { case \u0026#39;TOGGLE\u0026#39;: return { on: !state.on }; default: return state; } } function useToggle() { const [state, dispatch] = useReducer(toggleReducer, { on: false }); const toggle = () =\u0026gt; dispatch({ type: \u0026#39;TOGGLE\u0026#39; }); return { on: state.on, toggle, }; } 添加State Reducer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 function useToggle({ reducer = toggleReducer } = {}) { const [state, dispatch] = useReducer(reducer, { on: false }); const toggle = () =\u0026gt; dispatch({ type: \u0026#39;TOGGLE\u0026#39; }); return { on: state.on, toggle, }; } // 使用默认行为 function App() { const { on, toggle } = useToggle(); return \u0026lt;button onClick={toggle}\u0026gt;{on ? \u0026#39;ON\u0026#39; : \u0026#39;OFF\u0026#39;}\u0026lt;/button\u0026gt;; } // 自定义 reducer function App() { const { on, toggle } = useToggle({ reducer(state, action) { if (action.type === \u0026#39;TOGGLE\u0026#39; \u0026amp;\u0026amp; on) { return state; // 阻止关闭 } return toggleReducer(state, action); } }); return \u0026lt;button onClick={toggle}\u0026gt;{on ? \u0026#39;ON\u0026#39; : \u0026#39;OFF\u0026#39;}\u0026lt;/button\u0026gt;; } 使用场景 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 function useForm({ reducer = formReducer } = {}) { const [state, dispatch] = useReducer(reducer, { values: {} }); const setFieldValue = (name, value) =\u0026gt; dispatch({ type: \u0026#39;SET_FIELD\u0026#39;, name, value }); return { values: state.values, setFieldValue, }; } // 自定义 reducer 添加日志 function LoggingForm() { const { values, setFieldValue } = useForm({ reducer(state, action) { console.log(\u0026#39;Action:\u0026#39;, action); const newState = formReducer(state, action); console.log(\u0026#39;New state:\u0026#39;, newState); return newState; } }); // 渲染逻辑... } State Reducer的实现基础在于 reducer函数签名的一致性：（state, action） =\u0026gt; void 也正因为多个reducer共享相同的函数签名， 所以State Reducer也可以和组合多个HOC那样实现composer模式(类似前面组合多个HOC一样)\n多个reducer组合 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 function composeReducers(...reducers) { return (state, action) =\u0026gt; reducers.reduce((currentState, reducer) =\u0026gt; reducer(currentState, action), state); } function useAdvancedToggle() { const { on, toggle } = useToggle({ reducer: composeReducers( toggleReducer, (state, action) =\u0026gt; { if (action.type === \u0026#39;TOGGLE\u0026#39; \u0026amp;\u0026amp; state.on) { // 添加额外限制 return state; } return state; }, (state, action) =\u0026gt; { // 添加日志 console.log(action); return state; } ) }); } 总结 其实上面的设计模式， 核心就是围绕 关注点分离， 比如Container \u0026amp; presention模式(比较简单，上面没有详细展开)以及Props render， Props collections模式都是如此 逻辑复用， 比如HOC以及State Reducer; 前提是可复用逻辑本身和目标是存在正交性的；当然严格以上来说逻辑复用其实也是关注点分离的一种表现形式， 把重复的逻辑和主逻辑抽离出来 上诉react设计模式之所以需要强调关注点分离的一个重要原因在于：React Hook本身在设计上， 将UI和逻辑紧密地糅在了一起（相对的Vue3将逻辑也就是script和UI进行了分离， 两种流派各有利弊），所以在设计复杂应用的时候又需要以某种方式实现关注点分离\n","date":"2025-08-13T07:45:54Z","permalink":"https://superjcd.github.io/p/react%E5%B8%B8%E7%94%A8%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E6%96%B0/","title":"React常用设计模式(新)"},{"content":"我们知道好的prompt会直接关系到LLM应用的质量，那么如何通过提示工程获得更好的Prompt会是一件非常重要且有意义的事情。\n在实现某种调优之前， 我们首先需要搞清楚一件事情， 如何去评估某一项对Prompt进行调优的工程是不是真的是有效的\n实现对Query Engine的评估 实现的方式说起来很简单，以QA场景为例， 假定我们有一个人为标注过的数据集，它包含Question列和Answer列（见下表）， 然后我们用LLM回答Question，得到一个预测回答， 然后对照真实Answer和预测Anwser是不是一致或者近似一致就好了。\nQuestion Answer How old is Biden now Biden is 81 years old \u0026hellip;. \u0026hellip; \u0026hellip;. \u0026hellip;. 当然这里的问题在于， 我如何廉价地获取类似上面的这种格式的数据集呢？要知道人工标注是非常昂贵的。\n一个聪明的方案是: 相较于基于问题给出回答， 何不如基于回答来获取问题呢？后者在实现上， 对LLM来说是比较容易的， 所以只要通过LLM来解析文本， 并基于文本内容来由LLM输出对应的问题， 我们就能够获得一个类似于人为标注过的\u0026quot;黄金数据集\u0026quot;(Golden Dataset)\n具体代码实现\u0026ndash;以llamaindex为例子 准备\n1 2 3 4 5 6 7 8 from llama_index.core import Settings from llama_index.llms.ollama import Ollama from llama_index.embeddings.huggingface import HuggingFaceEmbedding llm = Ollama(model=\u0026#34;llama3\u0026#34;, request_timeout=60.0, temperature=0.1) Settings.llm = llm embed_modle = HuggingFaceEmbedding(\u0026#34;BAAI/bge-base-en-v1.5\u0026#34;) Settings.embed_model = embed_modle 老样子， 贫穷的我依然选择llama3开局 🤡（理想情况下肯定是上最新版本的chatgpt会更好）\n获取数据:\n1 2 3 4 5 6 7 from llama_index.readers.file import PDFReade from llama_index.core import Document loader = PDFReader() docs0 = loader.load_data(file=\u0026#34;./data/llama2.pdf\u0026#34;) doc_text = \u0026#34;\\n\\n\u0026#34;.join([d.get_content() for d in docs0]) docs = [Document(text=doc_text)] llama2.pdf是llama2的论文，下载地址：https://arxiv.org/pdf/2307.09288\n1 2 3 4 from llama_index.core.node_parser import SentenceSplitter node_parser = SentenceSplitter(chunk_size=1024) base_nodes = node_parser.get_nodes_from_documents(docs) 这里我们对文档按句子进行切分\n1 2 3 4 5 from llama_index.core import VectorStoreIndex index = VectorStoreIndex(base_nodes) query_engine = index.as_query_engine(similarity_top_k=2) 生成模拟的\u0026quot;黄金数据集\u0026quot;\n1 2 3 4 5 6 7 8 9 10 11 12 13 import nest_asyncio nest_asyncio.apply() dataset_generator = DatasetGenerator( base_nodes[:20], show_progress=True, num_questions_per_chunk=3, ) eval_dataset = await dataset_generator.agenerate_dataset_from_nodes(num=60) nest_asyncio.apply()是为了方便我们在jupyterlab中运行异步代码， 上面的例子其实不需要（因为jupyterlab本身就是运行在一个eventloop上的）， 但是后面会用到\neval_dataset就是我们通过LLM获取的数据集， 如果你还是很好奇DatasetGenerator实现原理， 你只要看一下它的prompt就知道了：\n如果你不感兴趣的话， 这部分可以跳过\n首先我们准备一个可以优美展示prompt的辅助函数：\n1 2 3 4 5 6 7 8 from IPython.display import Markdown, display def display_prompt_dict(prompts_dict): for k, p in prompts_dict.items(): text_md = f\u0026#34;**Prompt Key**: {k}\u0026lt;br\u0026gt;\u0026#34; f\u0026#34;**Text:** \u0026lt;br\u0026gt;\u0026#34; display(Markdown(text_md)) print(p.get_template()) display(Markdown(\u0026#34;\u0026lt;br\u0026gt;\u0026lt;br\u0026gt;\u0026#34;)) 然后查看dataset_generator的prompt:\n1 display_prompt_dict(dataset_generator.get_prompts()) 输出：\n1 2 3 4 5 6 7 Context information is below. --------------------- {context_str} --------------------- Given the context information and not prior knowledge. generate only questions based on the below query. {query_str} See？我们其实是让大模型根据文本来生成问题\n使用Meta Prompt来调优prompt 首先这个idea来自于这篇论文：https://arxiv.org/pdf/2309.03409\n至于实现， 简单来讲， 就是通过迭代的方式， 让Meta Prompt(元提示)基于现有的prompt表现(是一个随着迭代不断扩展的列表， 它会成为Meta Prompt的一部分来做为上下文的一部分)， 来生成更好的提示来改善我们的初始prompt\n首先我们来看一下我们的元提示长什么样：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 meta_tmpl_str = \u0026#34;\u0026#34;\u0026#34;\\ Your task is to generate the instruction \u0026lt;INS\u0026gt;. Below are some previous instructions with their scores. The score ranges from 1 to 5. {prev_instruction_score_pairs} Below we show the task. The \u0026lt;INS\u0026gt; tag is prepended to the below prompt template, e.g. as follows: ###### \u0026lt;INS\u0026gt; {prompt_tmpl_str} ####### The prompt template contains template variables. Given an input set of template variables, the formatted prompt is then given to an LLM to get an output. Some examples of template variable inputs and expected outputs are given below to illustrate the task. **NOTE**: These do NOT represent the \\ entire evaluation dataset. {qa_pairs_str} We run every input in an evaluation dataset through an LLM. If the LLM-generated output doesn\u0026#39;t match the expected output, we mark it as wrong (score 0). A correct answer has a score of 1. The final \u0026#34;score\u0026#34; for an instruction is the average of scores across an evaluation dataset. Write your new instruction (\u0026lt;INS\u0026gt;) that is different from the old ones and has a score as high as possible. Instruction (\u0026lt;INS\u0026gt;): \\ \u0026#34;\u0026#34;\u0026#34; 这一长串的promt的最终目的是为了生成一个promt的前缀（也就是上面元提示中的Instruction ()）， 后续会不断地根据生成的Instruction及相应的表现（这个表现就是基于前面我们模拟生成的那个数据集来的）\n首先我们需要准备好用来评估LLM问答效果的函数：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 from llama_index.core.evaluation.eval_utils import get_responses from llama_index.core.evaluation import CorrectnessEvaluator, BatchEvalRunner evaluator_c = CorrectnessEvaluator() evaluator_dict = { \u0026#34;correctness\u0026#34;: evaluator_c, } batch_runner = BatchEvalRunner(evaluator_dict, workers=2, show_progress=True) async def get_correctness(query_engine, eval_qa_pairs, batch_runner): eval_qs = [q for q, _ in eval_qa_pairs] eval_answers = [a for _, a in eval_qa_pairs] pred_responses = get_responses(eval_qs, query_engine, show_progress=True) eval_results = await batch_runner.aevaluate_responses( eval_qs, responses=pred_responses, reference=eval_answers ) avg_correctness = np.array( [r.score for r in eval_results[\u0026#34;correctness\u0026#34;]] ).mean() return avg_correctness 可以看到get_correctness会通过新的query_engine(内嵌了新生成的提示前缀, 也就是Instruction)来对我们的模拟数据中的问题进行回答， 然后基于这个预测回答和模拟数据中的回答来获得准确率\n1 2 3 4 5 from llama_index.core import PromptTemplate QA_PROMPT_KEY = \u0026#34;response_synthesizer:text_qa_template\u0026#34; meta_tmpl = PromptTemplate(meta_tmpl_str) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def format_meta_tmpl( prev_instr_score_pairs, prompt_tmpl_str, qa_pairs, meta_tmpl, ): \u0026#34;\u0026#34;\u0026#34;Call meta-prompt to generate new instruction.\u0026#34;\u0026#34;\u0026#34; # format prev instruction score pairs. pair_str_list = [ f\u0026#34;Instruction ():\\n{instr}\\nScore:\\n{score}\u0026#34; for instr, score in prev_instr_score_pairs ] full_instr_pair_str = \u0026#34;\\n\\n\u0026#34;.join(pair_str_list) # now show QA pairs with ground-truth answers qa_str_list = [ f\u0026#34;query_str:\\n{query_str}\\nAnswer:\\n{answer}\u0026#34; for query_str, answer in qa_pairs ] full_qa_pair_str = \u0026#34;\\n\\n\u0026#34;.join(qa_str_list) fmt_meta_tmpl = meta_tmpl.format( prev_instruction_score_pairs=full_instr_pair_str, prompt_tmpl_str=prompt_tmpl_str, qa_pairs_str=full_qa_pair_str, ) return fmt_meta_tmpl prev_instr_score_pairs是之前迭代产出的提示前缀以及相应的评分， 所以它会随着迭代的增加而不断扩展， 它的长度会和迭代次数对应的\n1 2 3 4 5 def get_full_prompt_template(cur_instr: str, prompt_tmpl): tmpl_str = prompt_tmpl.get_template() new_tmpl_str = cur_instr + \u0026#34;\\n\u0026#34; + tmpl_str new_tmpl = PromptTemplate(new_tmpl_str) return new_tmpl get_full_prompt_template其实就是把我们生成的Intruction和原有的提示进行了拼接， 后续会拿这个拼接完的提示去更新Intruction\n最后就是我们的主流程代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def _parse_meta_response(meta_response: str): return str(meta_response).split(\u0026#34;\\n\u0026#34;)[0] async def optimize_prompts( query_engine, initial_instr: str, base_prompt_tmpl, meta_tmpl, meta_llm, batch_eval_runner, eval_qa_pairs, exemplar_qa_pairs, num_iterations: int = 5, ): prev_instr_score_pairs = [] base_prompt_tmpl_str = base_prompt_tmpl.get_template() cur_instr = initial_instr for idx in range(num_iterations): if idx \u0026gt; 0: # first generate fmt_meta_tmpl = format_meta_tmpl( prev_instr_score_pairs, base_prompt_tmpl_str, exemplar_qa_pairs, meta_tmpl, ) meta_response = meta_llm.complete(fmt_meta_tmpl) print(fmt_meta_tmpl) print(str(meta_response)) # Parse meta response cur_instr = _parse_meta_response(meta_response) # append instruction to template new_prompt_tmpl = get_full_prompt_template(cur_instr, base_prompt_tmpl) query_engine.update_prompts({QA_PROMPT_KEY: new_prompt_tmpl}) avg_correctness = await get_correctness( query_engine, eval_qa_pairs, batch_runner ) prev_instr_score_pairs.append((cur_instr, avg_correctness)) # find the instruction with the highest score max_instr_score_pair = max( prev_instr_score_pairs, key=lambda item: item[1] ) # return the instruction return max_instr_score_pair[0], prev_instr_score_pairs 然后正式地开始迭代\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 new_instr, prev_instr_score_pairs = await optimize_prompts( query_engine, initial_instr, base_qa_prompt, meta_tmpl, llm, # note: treat llm as meta_llm batch_runner, eval_qr_pairs, exemplar_qr_pairs, num_iterations=5, ) new_qa_prompt = query_engine.get_prompts()[QA_PROMPT_KEY] print(new_qa_prompt) 最后输出的new_qa_prompt应该就是评分最好的那个prompt了， 当然我们也可以打印prev_instr_score_pairs，看看它是不是最好的prompt\n参考 https://docs.llamaindex.ai/en/stable/examples/prompts/prompt_optimization\n","date":"2024-04-30T00:00:00Z","permalink":"https://superjcd.github.io/p/%E9%AB%98%E7%BA%A7prompt%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A7-%E5%9F%BA%E4%BA%8E%E5%85%83%E6%8F%90%E7%A4%BA/","title":"高级Prompt优化技巧-基于元提示"},{"content":"先简单讲一下什么是RAG(Retrieval-Augmented Generation), 或者说为什么需要RAG。\nLLM的训练数据是基于过去的信息， LLM没有办法回答最新的问题， 比如你没有办法让一个2022年训练的模型回答2023年NBA的冠军归属。\n当然我们可以选择给他喂最新的数据重新训练大模型， 但是这种方式的成本非常高昂。\n如何教\u0026quot;过时\u0026quot;的模型以新知识， 通过RAG， 我们把新的知识灌输或者填充到对话的语境中， 那么LLM通过检索这些新知识， 结合模型原有的理解能力， 他就能够回答自己在训练时没有遇到过的问题。\n本教程我们会使用llamaindex，具体版本信息：\n1 2 3 4 5 6 7 Name: llama-index Version: 0.10.31 Summary: Interface between LLMs and your data Home-page: https://llamaindex.ai Author: Jerry Liu Author-email: jerry@llamaindex.ai License: MIT 基础RAG流程 这里我们会使用llama3而不是OpenAI的chatgpt来作为我们的基座模型， 关于如何部署和使用llama3可以参考ollama的github文档：\n首先， 我们会为我们的RAG应用准备好llm基座和词嵌入模型：\n1 2 3 4 5 6 7 8 9 10 from llama_index.core import Settings from llama_index.llms.ollama import Ollama from llama_index.embeddings.huggingface import HuggingFaceEmbedding from llama_index.core import SimpleDirectoryReader llm = Ollama(model=\u0026#34;llama3\u0026#34;, request_timeout=60.0, temperature=0.1) Settings.llm = llm embed_modle = HuggingFaceEmbedding(\u0026#34;BAAI/bge-base-en-v1.5\u0026#34;) Settings.embed_model = embed_modle Settings是全局的配置类， 这样可以免去后续显式输入llm参数以及embed_model参数的过程；这里我们用智源（BAAI）的bge通用模型, 他们的词嵌入模型是比较好的\n接着我们要加载文档， 并对文档进行分割\n1 2 3 4 5 6 7 8 9 10 11 from llama_index.core import SimpleDirectoryReader from llama_index.core.node_parser import SimpleNodeParser documents = SimpleDirectoryReader( input_files=[\u0026#34;./data/paul_graham_eassay.txt\u0026#34;] ).load_data() node_parser = SimpleNodeParser(chunk_size=1024) nodes = node_parser.get_nodes_from_documents(documents) print(len(nodes)) 我们读取了data路径下的paul_graham_eassay.txt文档， 并将文档分割成了长度为1024的若干个文档， 这样文档数从原来的1个变成了21个 打印其中一个文档：\n1 print(str(nodes[0])) 输出：\n1 2 3 4 5 6 7 (\u0026#39;Node ID: 00886ada-1a64-47ea-9f91-0eb48caf2138\\n\u0026#39; \u0026#39;Text: What I Worked On February 2021 Before college the two main\\n\u0026#39; \u0026#39;things I worked on, outside of school, were writing and programming. I\\n\u0026#39; \u0026#34;didn\u0026#39;t write essays. I wrote what beginning writers were supposed to\\n\u0026#34; \u0026#39;write then, and probably still are: short stories. My stories were\\n\u0026#39; \u0026#39;awful. They had hardly any plot, just characters with strong feelings,\\n\u0026#39; \u0026#39;whic...\u0026#39;) 在我正式使用语义进行文档查询之前， 我们需要准备好我们的向量数据库， 可选项有很多， 具体可以参考这里;这里我们使用weaviate(推荐使用docker本地运行weaviate， 具体参考这里, )\n1 2 3 4 5 6 7 8 9 10 11 import weaviate # v3 version from llama_index.core import VectorStoreIndex, StorageContext from llama_index.vector_stores.weaviate import WeaviateVectorStore index_name = \u0026#34;MyExternalContext\u0026#34; client = weaviate.Client(url=\u0026#34;http://localhost:8080\u0026#34;) vector_store = WeaviateVectorStore(weaviate_client=client, index_name=index_name) storage_context = StorageContext.from_defaults(vector_store=vector_store) index = VectorStoreIndex(nodes, storage_context=storage_context) 接着我使用VectotreStoreIndex作为我们查询引擎来查询文档：\n1 2 3 4 5 6 7 Question = \u0026#34;What happened at Interleaf?\u0026#34; query_engine = index.as_query_engine() response = query_engine.query(Question) print(response) 前置过程优化 \u0026ndash; 使用SentenceWindowNodeParser分割文档 所谓的前置优化， 就是发生在query之前的优化。\n前面我们使用了SimpleNodeParser来分割我们的文档， 当时我们把文档分割成了长度为1024的21个文档， 这里的问题在于：当我们进行query的时候， 我们会先定位到这21个文档中的某一个来填充llm的context， 然后通过llm来输出问题；\n很显然， 1024这个长度其实有点太大了，不利于精准定位；但是假设我缩小文档的长度，文档匹配的精度会上升， 但是同时可能导致context的上下文信息变少，也利于输出正确的答案；\n所以我们需要在匹配精度和保证context上下文的基础上进行平衡， 这里我们使用 SentenceWindowNodeParser来分割文档：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from llama_index.core.node_parser import SentenceWindowNodeParser node_parser = SentenceWindowNodeParser.from_defaults( window_size=3, window_metadata_key=\u0026#34;window\u0026#34;, # 会成为元数据 original_text_metadata_key=\u0026#34;original_text\u0026#34;, ) nodes = node_parser.get_nodes_from_documents(documents) print(len(nodes)) vector_store = WeaviateVectorStore(weaviate_client=client, index_name=index_name) storage_context = StorageContext.from_defaults(vector_store=vector_store) index = VectorStoreIndex(nodes, storage_context=storage_context) # 考虑文 同时我们需要去置换as_query_engine的node_postprocessors参数\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from llama_index.core.postprocessor import MetadataReplacementPostProcessor # The target key defaults to `window` to match the node_parser\u0026#39;s default postproc = MetadataReplacementPostProcessor( target_metadata_key=\u0026#34;window\u0026#34; ) query_engine = index.as_query_engine( node_postprocessors = [postproc], ) response = query_engine.query(Question) print(response) 匹配机制优化\u0026ndash;使用Hybrid Search 前面的优化方式， 发生在匹配之前， 我们也可以选择优化匹配算法， 将基于语义（词向量）的方式修改为结合关键词查询\n关键词查询的特点会查找和查询内容更相似的内容， 比如用户搜索apple， 在关键词查询的场景下， 所有带有apple的文档都是目标文档， 不论apple指的是苹果还是手机。 另外假设用户输入的词语发生了拼写错误， 关键词匹配机制也会失效\n1 2 3 4 5 6 7 8 9 query_engine = index.as_query_engine( node_postprocessors = [postproc], vector_store_query_mode=\u0026#34;hybrid\u0026#34;, alpha=0.5 ) response = query_engine.query(Question) print(response) alpha是0-1之间的值，表示关键词匹配的比例， 如果是0的话， 则不使用；\n注意并不是所有向量数据库都支持关键词匹配\n后置过程优化 最后， 当我们得到了目标结果， 如果我们想要进步一步优化查询， 那该怎么处理？ 一种方法就是使用reranker从候选文档（比如有6个候选文档）中筛选出1-2个更匹配的文档：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 from llama_index.core.postprocessor import SentenceTransformerRerank # 计算相似度 rerank = SentenceTransformerRerank(top_n=2, model=\u0026#34;BAAI/bge-reranker-base\u0026#34;) query_engine = index.as_query_engine( similarity_top_k=6, node_postprocessors = [rerank, postproc], vector_store_query_mode=\u0026#34;hybrid\u0026#34;, alpha=0.3 ) response = query_engine.query(Question) print(response) 参考 Retrieval-Augmented Generation (RAG): From Theory to LangChain Implementation\n","date":"2024-04-29T00:00:00Z","permalink":"https://superjcd.github.io/p/rag%E8%BF%9B%E9%98%B6%E6%8A%80%E5%B7%A7/","title":"RAG进阶技巧"},{"content":" 关联项目: https://github.com/superjcd/gocrawler\n缘起 我自己是一个python的重度使用者， python中有大量的爬虫相关的包， 也不乏类似scrapy这种非常有知名度的包， 结合redis的scrapy-redis似乎也能在一定程度上解决分布式爬虫的需求。 但是这些依然还是不够， 不管是使用用基于线程的爬虫框架（scrapy）， 还是基于协程的爬虫(ruia)， 它们还是不够高效， 当然一部分的原因是在于python语言本身。python的优势是灵活性， 以及丰富的生态， 但是如果要追求极致的运行效率， 那么go其实是一个更好的选择\n核心特性 个人向来推崇极简主义， 爬虫框架gocrawler的开发也不例外， 对我个人而言， 这个框架最最重要的其实就是一个任务调度器(scheduler)：\n1 2 3 4 5 6 7 8 9 package scheduler import \u0026#34;github.com/superjcd/gocrawler/request\u0026#34; type Scheduler interface { Pull() *request.Request Push(typ int, reqs ...*request.Request) Schedule() } scheduler需要实现的方法无非就是：\nPull, 获取一个request对象 Push, 提交若干request对象 Schedule， 调度器的入口， 启动并持续运行调度器 同样的, 以数据的存储模块store为例， store只需要实现一个Save方法就可以了\n1 2 3 4 5 6 7 8 9 package store import ( \u0026#34;github.com/superjcd/gocrawler/parser\u0026#34; ) type Storage interface { Save(ctx context.Context, datas ...parser.ParseItem) error } 其它组件， 再比如Fetcher(请求数据模块)也不过只是一个仅仅有Fetch方法的interface。\n1 2 3 type Fetcher interface { Fetch(ctx context.Context, req *request.Request) (*http.Response, error) } 当然gocrawler会为用户提供一些开箱可用的调度器以及存储组件等等， 但所有这些构成爬虫的组成部分， 不过就是一个个的接口， 用户可以使用gocrawler提供的模块， 也可以使用自己的， 所以gocrawler天然的秉承了极简主义和面向接口编程的哲学。\n极简和可拓展性才是gocrawler最最核心的feature\n构建请求对象 Request请求对象是一个结构体， 定义如下：\n1 2 3 4 5 6 7 8 type Request struct URL string Method string Retry int Data map[string]string } type RequestDataCtxKey struct{} Request对象字段说明：\nURL定义了请求地址 Method定义了具体的请求方法， 比如GET, POST等 Retry定义了重试的次数， 如果worker定义了全局最大retry数， 爬虫引擎会在关键节点(比如决定是否重试)的时候查看这个值 Data是用户自定义的一些元数据， 这些元数据会通过context.WithValue的方式注入到爬虫上下文中， 注入的键为上面的RequestDataCtxKey类型 为了实现爬虫的去重， 一种实现的方式是：不希望相同的Request在一定的时间内被访问两次， 首先需要去hash这个请求\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 func (r *Request) Hash(hashFields ...string) string { components := make([][]byte, 2+len(hashFields)) components[0] = []byte(r.URL) components[1] = []byte(r.Method) for i, field := range hashFields { if fieldValue, ok := r.Data[field]; ok { components[i+2] = []byte(fieldValue) } else { panic(fmt.Errorf(\u0026#34;field not in request.Data\u0026#34;)) } } hash := md5.Sum(bytes.Join(components, []byte(\u0026#34;:\u0026#34;))) return string(hash[:]) } 默认的Hash实现比较简单， 这里使用md5对指定的字段进行摘要；\n当然， 需要注意的一点是：什么时候设置已请求已访问很关键， 假设在获得请求的时候就进行设置， 那么如果请求失败没有入库， 依然会被认为是访问过的，所以在gocrawler中， 设置去重项的时间节点是在数据导入之后\n实现调度器 调度器是gocrawler的核心，因为需要实现分布式调度， 所以类似scrapy的那种基于线程和内存队列的方式是不合适的, 所以需要选择一个高吞吐的存储介质； 消息队列是很容易想到的选项， kafaka, rabbit mq, rocket mq这些其实都可以， 但是作为爬虫worker的消息队列， 不需要太强的一致性保证，所以gocrawler默认提供了一个基于nsq的实现：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 package nsq import ( \u0026#34;encoding/json\u0026#34; \u0026#34;log\u0026#34; \u0026#34;github.com/nsqio/go-nsq\u0026#34; \u0026#34;github.com/superjcd/gocrawler/request\u0026#34; \u0026#34;github.com/superjcd/gocrawler/scheduler\u0026#34; ) const ( DIRECT_PUSH = iota NSQ_PUSH ) type nsqScheduler struct { workerCh chan *request.Request nsqLookupdAddr string topicName string channelName string nsqConsumer *nsq.Consumer nsqProducer *nsq.Producer } type nsqMessageHandler struct { s *nsqScheduler } func (h *nsqMessageHandler) HandleMessage(m *nsq.Message) error { var err error if len(m.Body) == 0 { return nil } processMessage := func(mb []byte) error { var req request.Request if err = json.Unmarshal(mb, \u0026amp;req); err != nil { return err } h.s.Push(DIRECT_PUSH, \u0026amp;req) return nil } err = processMessage(m.Body) return err } var _ scheduler.Scheduler = (*nsqScheduler)(nil) func NewNsqScheduler(topicName, channelName, nsqAddr, nsqLookupdAddr string) *nsqScheduler { nsqConfig := nsq.NewConfig() nsqConsumer, err := nsq.NewConsumer(topicName, channelName, nsqConfig) if err != nil { log.Fatal(err) } nsqProducer, err := nsq.NewProducer(nsqAddr, nsqConfig) if err != nil { log.Fatal(err) } workerCh := make(chan *request.Request) return \u0026amp;nsqScheduler{workerCh: workerCh, topicName: topicName, channelName: channelName, nsqLookupdAddr: nsqLookupdAddr, nsqConsumer: nsqConsumer, nsqProducer: nsqProducer, } } func (s *nsqScheduler) Pull() *request.Request { req := \u0026lt;-s.workerCh return req } func (s *nsqScheduler) Push(typ int, reqs ...*request.Request) { switch typ { case DIRECT_PUSH: for _, req := range reqs { s.workerCh \u0026lt;- req } case NSQ_PUSH: for _, req := range reqs { msg, err := json.Marshal(req) if err != nil { log.Printf(\u0026#34;push msg to nsq failed\u0026#34;) } s.nsqProducer.Publish(s.topicName, msg) } default: log.Fatal(\u0026#34;wrong push type\u0026#34;) } } func (s *nsqScheduler) Schedule() { s.nsqConsumer.AddHandler(\u0026amp;nsqMessageHandler{s: s}) if err := s.nsqConsumer.ConnectToNSQLookupd(s.nsqLookupdAddr); err != nil { log.Fatal(err) } } func (s *nsqScheduler) Stop() { s.nsqConsumer.Stop() } 因为调度器需要同时实现Push和Pull的操作， 所以nsqScheduler同时具有一个nsq.Consumer(实现pull)和nsq.Producer(实现push)的指针. Pull的逻辑很简单， 直接从workerChan获取一个Request对象就好 Push则会根据类型选择DIRECT_PUSH直接往workerChan放一个Request， 如果是NSQ_PUSH则会是将Request通过nsqProducer发布到nsq里面 最后Schedule的实现很简单， 就是连接nsqlookupd, 开起监听， 当监听一个到message时就会触发HandleMessage回调函数， 而HandleMessage的主要逻辑就把获取的Request对象push到workerCh中\n由于nsq本身并不能保证顺序, 所以这个Scheduler不是严格意义上的FIFO, 当然对于爬虫来说绝对的先入先出不是特别的重要\n实现存储器 通常我们会使用mysql之类的关系性数据库作为存储器， 无它， 比较便利。但是考虑到爬虫任务的特殊性， 我个人其实会更倾向于使用文档性数据库， 因为页面的变动是常有的事， 而且，爬取得的数据不见得都是规则的，所以有严格的shema的关系型数据库显然不是最适合的选择\n虽然mysql现在也支持存储json， 但是查询性能无法与原生的文档数据库相媲美。当然使用gocrawler, 因为只是依赖接口， 所以实现一个支持Save的mysql存储器也是可以的。\ngocrawler自带基于mongo的存储器：\n无缓存mongo存储器 带缓存的mongo存储器 无缓存实现：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 type mongoStorage struct { Cli *qmgo.QmgoClient } var _ store.Storage = (*mongoStorage)(nil) func NewMongoStorage(uri, database, collection string) *mongoStorage { ctx := context.Background() cli, err := qmgo.Open(ctx, \u0026amp;qmgo.Config{Uri: uri, Database: database, Coll: collection}) // counter if err != nil { panic(err) } return \u0026amp;mongoStorage{Cli: cli} } func (s *mongoStorage) Save(items ...parser.ParseItem) error { var result *qmgo.InsertOneResult var err error for _, item := range items { result, err = s.Cli.Collection.InsertOne(context.Background(), item) if err == nil { log.Println(\u0026#34;[store]insert one ok\u0026#34;) } } if err != nil { return err } _ = result return nil } 无缓存的mongoStorage， 会在每次调用Save方法的时候， 依次将数据插入到mongodb中。 很显然这种实现只适合对爬虫速度要求不高的场景， 更合理的方式是使用缓存， 下面是带缓存的存储器实现(部分代码):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 type bufferedMongoStorage struct { L *sync.Mutex Cli *qmgo.QmgoClient buf []parser.ParseItem bufSize int counter counter.Counter taskKeyField string } // 略 func (s *bufferedMongoStorage) Save(items ...parser.ParseItem) error { s.L.Lock() defer s.L.Unlock() for { if len(items) \u0026gt; s.bufSize { return fmt.Errorf(\u0026#34;number of items too large(larger than the max bufSize), either increase storage bufSize or decrease number of items\u0026#34;) } if len(items) \u0026gt; (s.bufSize - len(s.buf)) { if err := s.flush(); err != nil { return err } } else { s.buf = append(s.buf, items...) break } } return nil } // 略 bufferedMongoStorage会对将要导入的数据进行判断：\n如果要导入的数据量大于最大缓存， 那么直接返会错误 如果要导入的数据量已经大于缓存的剩余空间， 那么直接促发flush， flush会将缓存buf中数据批量导入到mongodb， flush完成后buf会被清空 否则， 直接追加到缓存中并退出循环 之所以使用for循环的意义在于， 上诉的第二步只是把缓存中的数据写到mongo， items还没有被写入， 所以需要执行下次操作， 将它们加入到缓存中\n由于flush的触发条件是缓存已满或者将满的情况， 那么如果没有新的数据导入到缓存中， 那么缓存中的数据就会一直驻留， 所以我们需要某种机制来解决数据驻留的情况：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 func NewBufferedMongoStorage(...) *bufferedMongoStorage { (略) ticker := time.NewTicker(autoFlushInterval) go func() { for t := range ticker.C { log.Printf(\u0026#34;auto flush triggered at %v\u0026#34;, t) store.flush() } }() return store } 解决的方案也很简单， 利用定时器定时触发flush即可\n实现分布式任务计数 以python的scrapy为例， 他会在将request加入队列的时候统计任务数量：\n1 2 3 4 5 6 7 8 9 10 11 12 13 def enqueue_request(self, request: Request) -\u0026gt; bool: if not request.dont_filter and self.df.request_seen(request): self.df.log(request, self.spider) return False dqok = self._dqpush(request) assert self.stats is not None if dqok: self.stats.inc_value(\u0026#34;scheduler/enqueued/disk\u0026#34;, spider=self.spider) else: self._mqpush(request) self.stats.inc_value(\u0026#34;scheduler/enqueued/memory\u0026#34;, spider=self.spider) self.stats.inc_value(\u0026#34;scheduler/enqueued\u0026#34;, spider=self.spider) return True 然后stats是存储在内存中的一个字典， 这种实现方式也存在以下问题：\n相比于请求数， 其实我更关心完成数量， 特别是当我需要重试请求， 或者有带缓存的存储器的时候；请求数量不是一个核心指标 更重要的一点是， 在分布式场景下 ，基于worker的内存计数显然是不行的 顺带提一嘴，scrapy的计数函数inc_value并没有锁， 当然原因在于它的schduler的入队操作并不是并发实现的 1 2 3 4 5 def inc_value( self, key: str, count: int = 1, start: int = 0, spider: Optional[Spider] = None ) -\u0026gt; None: d = self._stats d[key] = d.setdefault(key, start) + count Ok, 言归正转，总而言之我需要一个可以在分布式场景下完成任务完成计数的功能， 一个很容易想到的解决方案就是redis的INCR操作, incr会以并发安全的方式为key加上1， 这样结合gocrawler的worker的AfterSave生命周期函数（添加一个调用redis的incr函数的hook）， 我就可以实现一个不错的分布式计数功能。\n但问题是， 如果我用的是带缓存的存储器， 然后实现的是批量存储呢？如果存储不会失败， 那么使用生命周期函数统计数量， 然后多次调用INCR似乎也不是不行， 但是不管是存储不会失败还是多次调用INCR都不是鲁棒性很高的操作，理想情况是在发生flush的情况下， 对flush的数量进行单次计数才是最完美的。但是redis除了incr是并发安全之外，常规的修改计数的方法一定会涉及get和put操作， 所以它不是并发安全的， 所以需要使用redis的事务：\n这里我直接用了官方示例， 稍微调整一下来实现了这个功能\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 ... func (c *redisTaskCounters) Incr(key string, increment int64) { // transaction key = c.prefix + key txf := func(tx *redis.Tx) error { // Get the current value or zero. n, err := tx.Get(key).Int64() if err != nil \u0026amp;\u0026amp; err != redis.Nil { return err } // Actual operation (local in optimistic lock). atomic.AddInt64(\u0026amp;n, increment) // Operation is commited only if the watched keys remain unchanged. _, err = tx.Pipelined(func(pipe redis.Pipeliner) error { pipe.Set(key, n, c.TTL) // time return nil }) return err } // Retry if the key has been changed. for i := 0; i \u0026lt; maxRetries; i++ { err := c.RCli.Watch(txf, key) if err == nil { // Success. return } if err == redis.TxFailedErr { // Optimistic lock lost. Retry. continue } // TODO: igonore any other error. return } } 既然这个计数功能需要统计flush的数据量， 那么我们就把它写在flush函数里面\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 func (s *bufferedMongoStorage) flush() error { if len(s.buf) == 0 { return nil } err := s.insertManyTOMongo(s.buf...) if err != nil { return err } if s.counter != nil { tc := s.collectTaskCounts(s.buf) s.count(tc) } // update buffer to an empty buffer s.buf = make([]parser.ParseItem, 0, s.bufSize) log.Printf(\u0026#34;Flushed\u0026#34;) return nil } func (s *bufferedMongoStorage) count(tc map[string]int64) { for k, v := range tc { s.counter.Incr(k, v) } } collectTaskCounts函数是考虑到缓存队列中可能同时存在多个任务的数据，所以考虑先统计了每个任务对应的数据量\n生命周期函数 目前gocrawler的worker支持的生命周期函数有：\nBeforeRequest AfterRequest BeforeSave AfterSave 它们的应用场景是怎样的呢？这里以BeforeRequest为例，\n1 2 3 4 5 6 7 8 9 10 type BeforeRequestHook func(context.Context, *request.Request) (Signal, error) func (w *worker) BeforeRequest(ctx context.Context, req *request.Request) (Signal, error) { var sig Signal if w.BeforeRequestHook != nil { return w.BeforeRequestHook(ctx, req) } sig |= DummySignal return sig, nil } BeforeRequest会在worker调用Fetch之前被调用：\n1 2 3 4 ... w.BeforeRequest(context.Background(), req) resp, err := w.Fetcher.Fetch(req) ... 因为BeforeRequestHook有指向Request的指针， 意味着我们可以在worker发起网络请求之前修改Request的参数， 比如Request的Url， 爬虫任务的目标url通常是规律的， 比如站点网址+ 某种ID或者页码数，而站点网址通常是固定的， 如此一来， 我们在提交任务的时候， 不用写入完整的url， 把补全Url的功能写成一个BeforeRequestHook提交给worker即可， 这样可以在很大程度减少任务队列的存储消耗\n关于Signal 生命周期函数的第一个返回值是Signal, Siganl是一个8位的flag， 方便用户告诉worker的主流程， 接下来该如何操作 通过signal, 用户可以控制worker选择直接进行一下轮循环， 或者直接忽略错误， 再或者直接panic错误\n生命周期函数是用户外部注入到框架的，所以通过Signal作为桥梁， 让用户拥有控制worker运行流程的能力是很重要的\n其他组件 其他诸如网页解析器， 代理获取， cookie获取， 请求头获取等组件的实现也并不困难, gocrawler同样也提供了相应的接口。 当然有一个组件其实非常重要，那就是进行网络请求的Fetcher组件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 type Fetcher interface { Fetch(req *request.Request) (*http.Response, error) } type fectcher struct { Cli *http.Client CookieGetter cookie.CoookieGetter UaGetter ua.UaGetter } func NewFectcher(timeOut time.Duration, proxyGetter proxy.ProxyGetter, cookieGetter cookie.CoookieGetter, uaGetter ua.UaGetter) *fectcher { tr := http.DefaultTransport.(*http.Transport) tr.Proxy = proxyGetter.Get tr.DisableKeepAlives = true client := \u0026amp;http.Client{Transport: tr, Timeout: timeOut} f := \u0026amp;fectcher{ Cli: client, CookieGetter: cookieGetter, UaGetter: uaGetter, } return f } func (f *fectcher) Fetch(r *request.Request) (resp *http.Response, err error) { jar, err := f.CookieGetter.Get() if err != nil { return nil, err } f.Cli.Jar = jar req, err := http.NewRequest(r.Method, r.URL, nil) if err != nil { return nil, fmt.Errorf(\u0026#34;get url failed: %w\u0026#34;, err) } ua, err := f.UaGetter.Get() if err != nil { return nil, fmt.Errorf(\u0026#34;get ua failed: %w\u0026#34;, err) } req.Header.Set(\u0026#34;User-Agent\u0026#34;, ua) resp, err = f.Cli.Do(req) if err != nil { return nil, err } return } gocrawler自带了一个基础的默认fetcher；可以看到NewFectcher的依赖基本上都是接口， 意味着用户可以自行替换所需要的依赖， 增加了框架的灵活性 默认fetcher会调用net/http中的请求函数来发起网络请求， 不过官方库的请求函数为了追求性能， 会复用连接， 但是很多爬虫场景下， 我们会希望我们每次的请求都是一个新的连接，这样就可以每次调用不同的代理， 所以在实例fetcher的时候， 我们把.DisableKeepAlives设置成了false\nworker完整工作流 有了前面的铺垫， 我们来看一下worker的Run入口函数的实现：\n1 2 3 4 5 6 7 8 9 10 11 12 func (w *worker) Run() { ctx, cancel := context.WithTimeout( context.Background(), w.MaxRunTime, ) defer cancel() for i := 0; i \u0026lt; w.Workers; i++ { go singleRun(w) } \u0026lt;-ctx.Done() } Run函数其实就是并发开启多个singleRun来执行具体的爬虫流程， 同时通过带TimeOut的上下文， 我们会让worker在MaxRunTime之后，退出主程序；最后附上singleRun的实现， 整体的逻辑还是比较简单的\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 func singleRun(w *worker) { for { w.Limiter.Wait(context.TODO()) req := w.Scheduler.Pull() if req == nil { continue } var reqKey string if w.UseVisit { if w.AddtionalHashKeys == nil { reqKey = req.Hash() } else { reqKey = req.Hash(w.AddtionalHashKeys...) } if w.Vister.IsVisited(reqKey) { continue } } originReq := req // Fetch w.BeforeRequest(context.Background(), req) resp, err := w.Fetcher.Fetch(req) if err != nil { log.Printf(\u0026#34;request failed: %v\u0026#34;, err) if req.Retry \u0026lt; w.MaxRetries { originReq.Retry += 1 w.Scheduler.Push(nsq.NSQ_PUSH, originReq) } else { log.Printf(\u0026#34;too many fetch failures for request:%s, exceed max retries: %d\u0026#34;, req.URL, w.MaxRetries) } continue } if resp.StatusCode != http.StatusOK { originReq.Retry += 1 w.Scheduler.Push(nsq.NSQ_PUSH, originReq) continue } // Parse parseResult, err := w.Parser.Parse(resp) if err != nil { log.Printf(\u0026#34;parse failed for request: %s, error: %v\u0026#34;, req.URL, err) originReq.Retry += 1 w.Scheduler.Push(nsq.NSQ_PUSH, originReq) continue } // New Requests if parseResult.Requests != nil \u0026amp;\u0026amp; len(parseResult.Requests) \u0026gt; 0 { for _, req := range parseResult.Requests { w.Scheduler.Push(nsq.NSQ_PUSH, req) } } // Save if parseResult.Items != nil \u0026amp;\u0026amp; len(parseResult.Items) \u0026gt; 0 { if w.SaveRequestData { for _, p_item := range parseResult.Items { for dk, dv := range req.Data { p_item[dk] = dv } } } w.BeforeSave(context.Background(), parseResult) if err := w.Store.Save(parseResult.Items...); err != nil { log.Printf(\u0026#34;item saved failed err: %v;items: \u0026#34;, err) continue } w.AfterSave(context.Background(), parseResult) } if w.UseVisit { w.Vister.SetVisitted(reqKey, w.VisterTTL) } } } 这里还需要追加提到的一点是， 如果用户决定使用过滤器（在一定时间周期内， 不想反复地抓取相同数据）， 那么可以通过下面的Option函数添加过滤器\n1 2 3 4 5 6 7 8 func WithVisiter(v visit.Visit, ttl time.Duration) Option { return func(opts *options) { opts.Vister = v opts.UseVisit = true opts.VisiterTTL = ttl } } worker的所有外部依赖都是基于Option模式添加的 然后如果UseVisit是true的话， worker会在发起请求前先判断Request是不是已经Visit过了, 如果开启UseVisit其有访问过， 那么就会直接进入下轮循环 gocralwer提供了一个基于redis的Vister的实现，默认实现并没有使用BloomFilter， 如果数据量非常大， 且不太在意部分Request会被误判为访问过(hash冲突)的话， 也可以自己实现一个基于BloomFilter(redis也是支持的)\n","date":"2024-02-21T00:00:00Z","permalink":"https://superjcd.github.io/p/golang%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB%E8%AE%BE%E8%AE%A1/","title":"golang分布式爬虫设计"},{"content":"动机 我们经常会看到很多的go语言库会使用go:generate xxx命令来减少一些重复性代码的编写（boilerplate codes）， 但是go:generate命令本身不是达到减轻重复性工作的关键， 关键是上面的xxx(指代任何一种代码生成工具)是如何帮组我们实现代码生成的呢？\n比如, 我们在写gin的controller的时候(MVC架构)， 我们通常会通过如下方式提取http请求对象的参数：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 // controller/controller.go type GetUserParmas struct { Name string `form:\u0026#34;name\u0026#34;` Email string `form:\u0026#34;email\u0026#34;` IsAdmin int `form:\u0026#34;is_admin\u0026#34;` } func (con *Controller) GetUser() gin.HandlerFunc { return func(c *gin.Context) { var rq GetUserParmas if err := c.ShouldBindQuery(\u0026amp;rq); err != nil { response.BadRequest(c, err) return } // } } 对Get请求我们我们会像上面那样调用ShouldBindQuery来解析参数， 问题的关键在于GetUserParams这个表示请求的结构体， 它在结构上其实和User结构体其实是类似的。\n我们在应用MVC的时候， 肯定会定义一些数据的模型， 如果使用gorm这类的ORM工具的话， 会直接借用这些结构体来进行对数据库的操作。但是在定义数据模型的时候， 用的tag可能会类似于:\n1 2 3 4 5 6 //models/users.go type User struct { Name string `gorm:\u0026#34;column:name\u0026#34;` Email string `gorm:\u0026#34;column:email\u0026#34;` IsAdmin int `gorm:\u0026#34;column:is_admin\u0026#34;` } 可以看到User和GetGetUserParams唯一的差别只是它们的tag是不同的而已， 上面演示的只是Get请求解析参数的场景， 如果还有类似的其他场景呢， 比如使用ShouldBindJson, 那么在我们的tag里面就需要加上json标签了， 很显然这个过程是很繁杂的。\n我们希望我们有一个聪明的工具， 可以帮助我们基于User以及其他的结构体， 自动生成生成GetUserParmas之类的代码， 减少重复的工作量.\n实现 实现的步骤大概分为：\n收集结构体 生成代码 收集结构体 首先我们需要收集到所有的结构体的相关信息，意味着需要知道结构体的名称， 结构体的字段， 字段类型， 字段标签等等， 譬如上面的User结构体中个字段信息。实现这个目标的方法大体上会有两种：\n反射 将代码解析成抽象语法树然后遍历 我们这里会使用第二种方法达到我们的目的。因为几乎所有的语言都会提供将源代码解析为目标语言AST树（抽象语法树）的工具， golang也不例外， golang的go/parser包中的ParseFile方法会将源码解析成AST树\n严格来讲， 使用正则表达式之类的方法应该也是可以的， 因为Parse代码的过程本身，通常也会包含某种形式的正则表达式来实现所谓的词法分析（lexical analyze）\n1 2 3 4 5 fset := token.NewFileSet() var contents interface{} node, err := parser.ParseFile(fset, file, contents, parser.ParseComments) 上面的file是一个源码文件名称，ParseFile会读取并把代码解析为ast.File对象， 也就是上面的node。\n接着我们会沿着ast.File这棵语法树进行遍历， 找到我们需要的结构体对象：\n1 structs := collectStructs(node) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 type structType struct { name string node *ast.StructType } func collectStructs(node ast.Node) []*structType { structs := make([]*structType, 0) collectStructs := func(n ast.Node) bool { var t ast.Expr var structName string switch x := n.(type) { case *ast.TypeSpec: if x.Type == nil { return true } structName = x.Name.Name t = x.Type case *ast.CompositeLit: t = x.Type case *ast.ValueSpec: structName = x.Names[0].Name t = x.Type case *ast.Field: if len(x.Names) != 0 { structName = x.Names[0].Name } t = x.Type } t = deref(t) x, ok := t.(*ast.StructType) if !ok { return true } structs = append(structs, \u0026amp;structType{ name: structName, node: x, }) return true } ast.Inspect(node, collectStructs) return structs } collectStructs的核心其实是通过ast.Inspect方法进行遍历AST树， 它的第二个参数是一个参数为ast.Node的回调函数， 回调函数如果返回true， 会继续遍历。\n上面的代码会遍历AST树的所有节点Node， 然后将目标对象ast.StructType收集起来返回; 注意, 我们这里其实自定义了一个structType对象， 原因在于ast.StructType没有包含结构体名称的字段， 而结构体名称后续是需要用到的\ncollectStructs函数参考了https://github.com/fatih/gomodifytags中的实现\n这里稍微插播一下ast.Inspect的实现：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 func Inspect(node Node, f func(Node) bool) { Walk(inspector(f), node) } func Walk(v Visitor, node Node) { if v = v.Visit(node); v == nil { return } // walk children // (the order of the cases matches the order // of the corresponding node types in ast.go) switch n := node.(type) { // Comments and fields case *Comment: // nothing to do case *CommentGroup: for _, c := range n.List { Walk(v, c) } case *Field: if n.Doc != nil { Walk(v, n.Doc) } walkIdentList(v, n.Names) if n.Type != nil { Walk(v, n.Type) } if n.Tag != nil { Walk(v, n.Tag) } if n.Comment != nil { Walk(v, n.Comment) } case *FieldList: for _, f := range n.List { Walk(v, f) } （中间略） // Files and packages case *File: if n.Doc != nil { Walk(v, n.Doc) } Walk(v, n.Name) walkDeclList(v, n.Decls) // don\u0026#39;t walk n.Comments - they have been // visited already through the individual // nodes case *Package: for _, f := range n.Files { Walk(v, f) } default: panic(fmt.Sprintf(\u0026#34;ast.Walk: unexpected node type %T\u0026#34;, n)) } 这里使用了递归下降的方法去遍历整棵树，通过对节点类型的判断， 使用不同的方法(Walk, walkDeclList等)来更深一步地进行递归调用。\n当我们收集完了所有结构体之后， 为了方便后续的代码生成环节， 我们这里对收集的结构体进行进一步的挖掘， 获取结构体中的字段名称、类型、Tag等信息：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 components, err := collectStructComponents(structs) func collectStructComponents(structs []*structType) (map[string]structComponets, error) { result := make(map[string]structComponets, len(structs)) for _, s := range structs { sc := structComponets{ Name: s.name, } fieldNames := make([]string, 0) fieldTypes := make([]string, 0) fieldTags := make([]string, 0) for _, f := range s.node.Fields.List { fieldName := \u0026#34;\u0026#34; if len(f.Names) != 0 { for _, field := range f.Names { fieldName = field.Name } } if f.Names == nil { ident, ok := f.Type.(*ast.Ident) if !ok { continue } fieldName = ident.Name } if fieldName == \u0026#34;\u0026#34; { continue } if f.Tag == nil { f.Tag = \u0026amp;ast.BasicLit{} } fieldNames = append(fieldNames, fieldName) // add Types typeExpr := f.Type typeString := types.ExprString(typeExpr) fieldTypes = append(fieldTypes, typeString) // add Tags tagExpr := f.Tag tagString := types.ExprString(tagExpr) fieldTags = append(fieldTags, tagString) } sc.FieldNames = fieldNames sc.FieldTypes = fieldTypes sc.FieldTags = fieldTags result[sc.Name] = sc } return result, nil } 代码整体比较简单， 就是遍历StructType的Fields.List来获取每个结构体字段的详细信息， 最后汇总成一个字典。\n生成代码 当我们有了所需的结构体信息之后， 生成代码就是一件非常水到渠成的事情; 这里我们会使用golang自带template包来实现代码生成\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 genCode(file, components) func genCode(file string, scm map[string]structComponets) { fileName := strings.TrimRight(file, \u0026#34;.go\u0026#34;) src := fmt.Sprintf(\u0026#34;package %s\u0026#34;, pkgName) for _, v := range scm { forms, err := genForm(v) if err != nil { log.Fatal(err) } src += \u0026#34;\\n\u0026#34; + forms jsons, err := genJson(v) if err != nil { log.Fatal(err) } src += \u0026#34;\\n\u0026#34; + jsons } finalSrc, err := format.Source([]byte(src)) if err != nil { log.Fatal(err) } // fmt.Println(string(finalSrc)) outFile := fmt.Sprintf(\u0026#34;%s_param.go\u0026#34;, fileName) os.WriteFile(outFile, finalSrc, 0644) } genCode会遍历我们上一步获取的字典， 然后调用genForm和genJson生成我们想要得到的目标代码:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 func ToSnakeCase(str string) string { snake := matchFirstCap.ReplaceAllString(str, \u0026#34;${1}_${2}\u0026#34;) snake = matchAllCap.ReplaceAllString(snake, \u0026#34;${1}_${2}\u0026#34;) return strings.ToLower(snake) } var ( matchFirstCap = regexp.MustCompile(\u0026#34;(.)([A-Z][a-z]+)\u0026#34;) matchAllCap = regexp.MustCompile(\u0026#34;([a-z0-9])([A-Z])\u0026#34;) funcMap = template.FuncMap{ \u0026#34;ToLower\u0026#34;: strings.ToLower, \u0026#34;ToSnake\u0026#34;: ToSnakeCase, } ) func genForm(sc structComponets) (string, error) { tempString := ` type {{.Name}}Form struct { {{range $i, $e := .FieldNames}} {{$e}}\t{{index $.FieldTypes $i}}` + \u0026#34; `form:\\\u0026#34;{{$e|ToSnake}}\\\u0026#34;`\u0026#34; + `{{end}}` + ` }` temp, err := template.New(\u0026#34;controller\u0026#34;).Funcs(funcMap).Parse(tempString) if err != nil { return \u0026#34;\u0026#34;, err } buff := bytes.NewBufferString(\u0026#34;\u0026#34;) err = temp.Execute(buff, sc) if err != nil { return \u0026#34;\u0026#34;, err } return buff.String(), nil } 上面以genForm为例， 我们使用template的Excute方法将Struct中的诸多信息注入到了代码模板中\n牛刀小试 上面的代码我已上传至github， 我们可以下载下来使用一下：\n1 go install github.com/superjcd/paramgen 然后我们在目标文件的最上方(譬如下面这样)， 添加//go:generate paramgen\n1 2 3 4 5 6 7 8 9 10 //go:generate paramgen package model type User struct { gorm.Model Name string Password string Email string IsAdmin int } 然后再根目录的终端运行：\n1 go generate ./... 然后就会看到上面文件的旁边新生成了后缀_param的go文件.\nReference the-ultimate-guide-to-writing-a-go-tool go-command-generate ","date":"2024-01-29T00:00:00Z","permalink":"https://superjcd.github.io/p/golang%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90/","title":"golang代码生成"},{"content":"Seq2Seq 模型架构 seq2seq是2014年随论文[Sequence to Sequence Learning with Neural Networks]发布 的一种NLG自然语言（生成式）模型， 下图直观展示了seq2seq模型在机器翻译任务（德译英）中，神经网络前向计算（Forwad Computiation）的工作流程\nseq2seq模型的关键点在于， 将模型分成了编码器和解码器：\n编码器在编码阶段，会对每一个Token（上面的话是一个个的单词）进行编码，编码过程通常会使用RNN来实现(论文里使用的是LSTM， 所以会同时有hidden state和cell两个输出， 如果是最原始的RNN， 那么应该就只有一个hidden state)。\n解码器会沿用编码器的结构， 比如这里的编码器用的是双层的LSTM(论文里面是4层)， 那么解码器也会使用双层的LSTM， 同时解码器的最初的hidden state和cell刚好就是解码器的最后一步产出的hidden state和cell\nhidden state的维度和cell的维度通常会一样\n编码器 我们拆解一下整个seq2seq， 先只关注编码器这个部分\n编码器前向计算涉及的公式：\ne指的是embedding词嵌入\n也就是当我们使用一般的RNN模型时， 就是上面的公式， 如果是LSTM, 因为会涉及cell（记忆单元）所以会多一个参数。上面的公式没有涉及层级， 如果涉及多个层级（只有第一层会用到embedding）， 以LSTM模型为例， 公式为：\n从第二层layer开始， 编码器的第一个参数变成了前一层的hidden state而不是embbeding\n编码器pytorch实现 1 2 3 4 5 6 7 8 9 10 11 12 13 class Encoder(nn.Module): def __init__(self, input_dim, embedding_dim, hidden_dim, n_layers, dropout): super().__init__() self.hidden_dim = hidden_dim self.n_layers = n_layers self.embedding = nn.Embedding(input_dim, embedding_dim) self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout) self.dropout = nn.Dropout(dropout) def forward(self, src): embedded = self.dropout(self.embedding(src)) outputs, (hidden, cell) = self.rnn(embedded) return hidden, cell 这里注意负责前向计算逻辑的forward函数中参数src的形状是：输入句子长度（或者说token数） * Bacth大小，因为LSTM会默认接受输入句子长度 * Bacth大小 * Embedding大小的输入， 虽然可以将Batch大小放到第一维度， 但是考虑到seq2seq其实是根据一个一个的token进行前向运算的运算的， 所以第一个维度是句子长度是更符合逻辑的， 相当于将token置于for loop的最外层（虽然可能不符合多数人的直觉）\nOk， 由于我们的Decoder只需要hidden state和cell， 所以forwar只输出hidden和cell\n解码器 回过头来我们来关注解码器：\n其实解码器和编码器在结构上是非常相似的， 一个比较直观的区别在于： 解码器会在每个计算步骤产出的output之上构建一个全连接层， 然后输出的大小是词汇表的大小\n解码器的计算公式：\n这里的d其实就是目标输出的embedding layer, 类比编码器公式中的e； s就是hidden state类比编码器公式中的h\n解码器pytorch实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class Decoder(nn.Module): def __init__(self, output_dim, embedding_dim, hidden_dim, n_layers, dropout): super().__init__() self.output_dim = output_dim self.hidden_dim = hidden_dim self.n_layers = n_layers self.embedding = nn.Embedding(output_dim, embedding_dim) self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout) self.fc_out = nn.Linear(hidden_dim, output_dim) self.dropout = nn.Dropout(dropout) def forward(self, input, hidden, cell): input = input.unsqueeze(0) embedded = self.dropout(self.embedding(input)) output, (hidden, cell) = self.rnn(embedded, (hidden, cell)) prediction = self.fc_out(output.squeeze(0)) return prediction, hidden, cell 在代码上， 解码器也是和编码器相似的（甚至rnn模型的参数也是一样的）， 当然也一些不同的地方：\nforward的参数多了hidden和cell， 这个是encoder的输出 input由于是一个个的token， 所以使用input = input.unsqueeze(0)补充了第一维度， 这是为了满足LSTM模型的输入需求 prediction预测是构建在output之上的， 由于是第一维可有可无， 我们这里用squeeze压缩掉 完整seq2seq代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 class Seq2Seq(nn.Module): def __init__(self, encoder, decoder, device): super().__init__() self.encoder = encoder self.decoder = decoder self.device = device assert encoder.hidden_dim == decoder.hidden_dim assert encoder.n_layers == decoder.n_layers def forward(self, src, trg, teacher_forcing_ratio): batch_size = trg.shape[1] trg_length = trg.shape[0] trg_vocab_size = self.decoder.output_dim outputs = torch.zeros(trg_length, batch_size, trg_vocab_size).to(self.device) hidden, cell = self.encoder(src) input = trg[0,:] for t in range(1, trg_length): output, hidden, cell = self.decoder(input, hidden, cell) outputs[t] = output teacher_force = random.random() \u0026lt; teacher_forcing_ratio top1 = output.argmax(1) input = trg[t] if teacher_force else top1 return outputs 这里先解释一下上面的几个新变量：\ntrg其实是target， 表示目标输出， 所以在结构上和input是相似的: 目标字符串长度(token数) * Batch大小 teacher_forcing_ratio， 其实就是一个0到1的数值（表示概率）， 这个值表示使用上一步的真实值（ground truth）或者预测值作为下一步输入的概率， 显然易见这个值如果太大的话可能会导致过拟合 当我们把encoder和decoder的逻辑放在一起的时候， 我们能清晰地看到forwad中有一个for循环会沿着输出长度一步一步调用decoder， 然后将每一步的output填充到最后的结果outputs中；\n注意， encoder在整个前向计算中只进行了一次！\n","date":"2024-01-18T00:00:00Z","permalink":"https://superjcd.github.io/p/seq2seq%E5%8E%9F%E7%90%86%E5%88%B0%E5%AE%9E%E7%8E%B0/","title":"Seq2Seq原理到实现"},{"content":"网络编程入门 同时也算是beej‘s guide to network programming的阅读笔记， 当然这里不一定会按章原书的章节原封不动的来， 会穿插一些个人见解（囿于水平有限， 无法保证都是对的）\n前置知识 什么是socket 在unix系统中， 所有的一切都是文件，这些文件会关联一个文件描述符（其实就是一个整数， 可以理解为这些文件的id）。\n当我们在unix系统中， 使用scoket()会返回一个文件描述符（文件id）, 然后我们通过send和recv对这个文件上进行了类似write和read的操作， 以此来达到交换数据的目的\nsocket类型 网络socket是有两种类型的：\nstream dgram(DATAGRAM) 前者使用的是TCP协议(Transmission Control Protocol), TCP协议在传输过程中是要一直保证链接的， 而且数据包是有序传递的(使用telnet就是顺序传输信息的)且会是完整传递；\n相反地datagram使用的是UDP（User Datagram Protocol）， 数据不能保证是有序的， 甚至不能保证数据是完整的（当然相应的， udp的传输可以更快）。\n有些文件传输软件， 比如tftp其实用的也是udp， 但是它在udp协议的基础上又添加了一层协议，这层协议会要求接收者受到信息后返回一个信号ACK,信息发送者只有收到这个信号才能表示发送信息成功， 不然信息会被重发\nData Encapsulation 我们知道OSI（open system interconnect ）会有7个层级：\nApplication Presentation Session Transport Network Data Link Physical 每一个层级都会携带一些相应的信息，最开始用户发送的出去的信息可能就是一个简单的字符串， 这一串信息会被层层封包：\n越外层的封包信息， 越接近物理层， 为什么？因为数据的接收者， 肯定先是物理层， 比如Ethernet， wifi这些靠近物理的layer， 最后层层解包， 拿到被封装在最里面的数据data\nipv4和ipv6 ipv4地址， 诸如： 192.10.2.111, 就是4字节， 32位的地址，也就是存着2的32次方的可能性， 大概最多能表示40亿个地址。\n在互联网刚刚诞生的时候， 40亿当然是个天文数字， 但是在人均都有手机以及各种可以联网的穿戴设备的今天， 40亿有点捉襟见肘了。\nipv6则是16个字节， 也就是128位， 意味着能表示2的128种地址， 所以很显然， 即便是全宇宙的生物体都有一台能连接互联网的手机， 应该也是够够用了。\nipv6的地址： 2001:0db8:c9d2:aee5:73e3:934a:a5ae:9551\n上面每个冒号分割的都是用16进制表示的2个字节， 也就是16位(16 * 8刚好就是128) 在ipv6在表示上， 为了更简便， 可以压缩每个冒号分割中靠前面的0， 比如上面的0db8这部分可以压缩成db8。\n类似0000:0000:0000:0000:0000:0000:0000:0001这样的地址可以直接被压缩位::1,这个就是相当于ipv4中locahost， 即127.0.0.1 。 另外ipv4是可以转化位ipv6， 转化规则并没有我们想象中的需要牵涉到进制的转化，而是直接在ipv4地址的基础上加上前缀， 比如地址192.0.2.33可以转化位 ::ffff:192.0.2.33\nsubnets子网 通常我们会把ip地址拆分成所谓network部分和host部分，假设我们的netmask是255.255.255.0 ，然后我们的ip地址是192.0.2.33， 那么这个地址的network部分就是192.0.2.0， 这个值就是子掩码和ip地址执行逻辑与的结果。\n一种更为灵活的表示方法是将ip地址直接表示为192.0.2.33/24 （也就是netmask是255.255.255.0）， ipv6也是同理\n字节顺序 字节序有两种， 一种是大端（Big-Endian）另外一种是小端（Little-Endian），大端字节序通常会把最有效位存储在较低的地址， 小端则相反。\n比如大端会把一个10进制数字100中的1， 先存储在内存的较低的位置。\n由于网络基本上是大端存储的， 而很多操作系统又是小端的， 所以在信息传输的过程中自然会涉及到字节序的转换。\n比如函数htons()， 这个函数就是用来把host的short格式的数据转化成network的short格式\n应用1-查找ip地址 代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 /* ** showip.c -- show IP addresses for a host given on the command line */ #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;string.h\u0026gt; #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;sys/socket.h\u0026gt; #include \u0026lt;netdb.h\u0026gt; #include \u0026lt;arpa/inet.h\u0026gt; #include \u0026lt;netinet/in.h\u0026gt; int main(int argc, char *argv[]) { struct addrinfo hints, *res, *p; // 1 int status; char ipstr[INET6_ADDRSTRLEN]; if (argc != 2) { fprintf(stderr,\u0026#34;usage: showip hostname\\n\u0026#34;); return 1; } memset(\u0026amp;hints, 0, sizeof hints); hints.ai_family = AF_UNSPEC; // 2 hints.ai_socktype = SOCK_STREAM; if ((status = getaddrinfo(argv[1], NULL, \u0026amp;hints, \u0026amp;res)) != 0) { // 3 fprintf(stderr, \u0026#34;getaddrinfo: %s\\n\u0026#34;, gai_strerror(status)); return 2; } printf(\u0026#34;IP addresses for %s:\\n\\n\u0026#34;, argv[1]); for(p = res;p != NULL; p = p-\u0026gt;ai_next) { // 4 void *addr; char *ipver; // get the pointer to the address itself, // different fields in IPv4 and IPv6: if (p-\u0026gt;ai_family == AF_INET) { // IPv4 struct sockaddr_in *ipv4 = (struct sockaddr_in *)p-\u0026gt;ai_addr; addr = \u0026amp;(ipv4-\u0026gt;sin_addr); ipver = \u0026#34;IPv4\u0026#34;; } else { // IPv6 struct sockaddr_in6 *ipv6 = (struct sockaddr_in6 *)p-\u0026gt;ai_addr; addr = \u0026amp;(ipv6-\u0026gt;sin6_addr); ipver = \u0026#34;IPv6\u0026#34;; } // convert the IP to a string and print it: inet_ntop(p-\u0026gt;ai_family, addr, ipstr, sizeof ipstr); printf(\u0026#34; %s: %s\\n\u0026#34;, ipver, ipstr); } freeaddrinfo(res); // free the linked list return 0; } 说明 说明的数值标识对应代码后面的数字注释\naddrinfo是一个特殊的结构体， 结构体的签名如下： 1 2 3 4 5 6 7 8 9 10 11 struct addrinfo { int ai_flags; // AI_PASSIVE, AI_CANONNAME, etc. int ai_family; // AF_INET, AF_INET6, AF_UNSPEC int ai_socktype; // SOCK_STREAM, SOCK_DGRAM int ai_protocol; // use 0 for \u0026#34;any\u0026#34; size_t ai_addrlen; // size of ai_addr in bytes struct sockaddr *ai_addr; // struct sockaddr_in or _in6 char *ai_canonname; // full canonical hostname struct addrinfo *ai_next; // linked list, next node }; 可以看到addrinfo除了会包含一些和网络地址， socket 类型等基础信息之外， 还有一个指向下一个地址的指针（ai_next）， ai_addr是一个指向sockaddr结构体的指针：\n1 2 3 4 struct sockaddr { unsigned short sa_family; // address family, AF_xxx char sa_data[14]; // 14 bytes of protocol address }; sa_data当然存储的是协议地址， 在实际代码编写的时候我们会使用sockaddr_in(ip4地址)或者sockaddr_in6(ipv6)， 然后转化为sockaddr， 这里一sockaddr_in为例， 它的结构是：\n1 2 3 4 5 6 struct sockaddr_in { short int sin_family; // Address family, AF_INET unsigned short int sin_port; // Port number struct in_addr sin_addr; // Internet address unsigned char sin_zero[8]; // Same size as struct sockaddr }; 上面的sin_family就是个枚举值，和addrinfo的ai_family的值其实是一致的。 然后是上面的in_addr：\n1 2 3 struct in_addr { uint32_t s_addr; // that\u0026#39;s a 32-bit int (4 bytes) }; 然后in_addr就是个4字节的地址信息（ipv4地址）\n这里我们没有限定是IPV4还是IPV6, 如果想要限定， 可以通过AF_INET， AF_INET6来限定IPV4或IPV6 getaddrinfo会返回一个整数值， 如果该值为0， 标识运行成功， 不然就是有错误 这里插一句： go语言的错误处理机制从某种意义上来讲， 沿袭了c语言， 也就是直接返回错误， 让调用者决定如何处理， 而不是抛出异常的这种java/python的方式\n这里我们进入了一个for循环， 前面我们有提及addrinfo本质上是一个链表，它有一个指向下一个addrinfo的指针， 我们不断循环获取addr地址， 然后用这个地址去获取ip地址的字符串（inet_ntop中的ntop实际为 network to presentation） 应用2-使用scoket进行网络传输 使用socket进行通行， 通常会牵涉到两个角色：\n接受信号的， 我们称之为server服务端 发送信号的， 我们称之为client客户端 server既然是接受信号的， 涉及的流程通常会有listen（监听）和recv（接受数据， 如果是udp的话就是recvto） client主要是发送信号的， 所以他通常会涉及connect（链接）和send（传送， 如果是udp的话就是sendto） 上面所有的操作首先是需要围绕调用socket函数返回的套接字描述符(socket descriptor)展开的 代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 #include \u0026lt;string.h\u0026gt; #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;sys/socket.h\u0026gt; #include \u0026lt;netdb.h\u0026gt; #define MYPORT \u0026#34;3490\u0026#34; // the port users will be connecting to #define BACKLOG 10 // how many pending connections queue will hold int main(void) { struct sockaddr_storage their_addr; socklen_t addr_size; struct addrinfo hints, *res; int sockfd, new_fd; // !! don\u0026#39;t forget your error checking for these calls !! // first, load up address structs with getaddrinfo(): memset(\u0026amp;hints, 0, sizeof hints); hints.ai_family = AF_UNSPEC; // use IPv4 or IPv6, whichever hints.ai_socktype = SOCK_STREAM; hints.ai_flags = AI_PASSIVE; // 1 getaddrinfo(NULL, MYPORT, \u0026amp;hints, \u0026amp;res); // make a socket, bind it, and listen on it: sockfd = socket(res-\u0026gt;ai_family, res-\u0026gt;ai_socktype, res-\u0026gt;ai_protocol); // 2 bind(sockfd, res-\u0026gt;ai_addr, res-\u0026gt;ai_addrlen); // 3 listen(sockfd, BACKLOG); // 4 // now accept an incoming connection: addr_size = sizeof their_addr; 、、 5 new_fd = accept(sockfd, (struct sockaddr *)\u0026amp;their_addr, \u0026amp;addr_size); //5 // ready to communicate on socket descriptor new_fd! . . . 说明 使用AI_PASSIVE作为我们ai_flags就是告诉程序我要绑定本机的ip， 这也就是是为什getaddrinfo的第一个参数可以是空的， 因为我是server，所以不需要特定的server name， 这里调用了socket函数， 返回一个套接字描述符， socket的三个参数， ai_family就是前面提到的ipv4或者ipv6， sockettype是前面提到的SOCKETSTREAM（tcp协议）或者SOCKETDGRAM（udp协议） bind使用了socket返回的描述符号， 然后会bind到一个具体的port 然后server调用liste开启监听， BACKLOG是一个常量用来控制能同事监听的最大数量(其实就是限制了监听队列的大小) accept其实就是把客户端的connect给accept的意思，然后accept会返回一个全新的套接字描述符号， 这个新的描述符就是后续用来send 应用3 - 简单的server \u0026amp; client实现 完整代码的话， 直接参考：https://beej.us/guide/bgnet/html/split/client-server-background.html\nserver端 整体的server的运行流程还是遵循了前面提到的几个关键点：\ngeraddtrinfo获取ip地址信息 socket 建立一个套接字链接返回socket descriptor bind 绑定特定port listen 监听 accept 接受一个incomming connection send 通过socket发送信息 不过这里为了同时处理更多的链接， 会用fork开启一个子进程：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 while(1) { // main accept() loop sin_size = sizeof their_addr; new_fd = accept(sockfd, (struct sockaddr *)\u0026amp;their_addr, \u0026amp;sin_size); if (new_fd == -1) { perror(\u0026#34;accept\u0026#34;); continue; } inet_ntop(their_addr.ss_family, get_in_addr((struct sockaddr *)\u0026amp;their_addr), s, sizeof s); printf(\u0026#34;server: got connection from %s\\n\u0026#34;, s); if (!fork()) { // this is the child process close(sockfd); // child doesn\u0026#39;t need the listener if (send(new_fd, \u0026#34;Hello, world!\u0026#34;, 13, 0) == -1) perror(\u0026#34;send\u0026#34;); close(new_fd); exit(0); } close(new_fd); // parent doesn\u0026#39;t need this } fork在成功开启子进程的时候， 返回值是0， 所以 !0 就是true; 但是这里没有fork失败的处理逻辑\n因为用到了子进程， 自然需要有子进程的垃圾回收\n1 2 3 4 5 6 7 sa.sa_handler = sigchld_handler; // reap all dead processes sigemptyset(\u0026amp;sa.sa_mask); sa.sa_flags = SA_RESTART; if (sigaction(SIGCHLD, \u0026amp;sa, NULL) == -1) { perror(\u0026#34;sigaction\u0026#34;); exit(1); } 当主进程通过sigaction获取到了SIGCHLD这个信号的时候， 会触发函数sigchld_handler来进行GC：\n1 2 3 4 5 6 7 8 9 void sigchld_handler(int s) { // waitpid() might overwrite errno, so we save and restore it: int saved_errno = errno; while(waitpid(-1, NULL, WNOHANG) \u0026gt; 0); errno = saved_errno; } client端 client的代码会比较简单， 它的流程和server有少许不一样：\ngetaddrinfo， 不过作为client，我的第一个参数是需要指定server name的 socket connect recv， 和服务端需要使用accept返回的新的socket descriptor来发送信息， client直接用socket返回的desciptor就可以接受信息了 That\u0026rsquo;s it ","date":"2024-01-01T00:00:00Z","permalink":"https://superjcd.github.io/p/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%85%A5%E9%97%A8/","title":"网络编程入门"},{"content":"第二章： B-Tree Basics B+ 树是几乎所有基于磁盘的关系型数据库(mysql， pg等)的基础数据结构 B+ 树是Binary Search Tree的衍生， 有跟多的fanout, 所以搜索速度会更快（Log2 -\u0026gt; LogK, 当然在它们的算法复杂度是一致的） B+树的搜索见下图， 是从root根节点一直往下遍历的 数据的增删会触发B+数的split和merge（触发条件就是overflow和underflow） 示意图 B+树（n key， n+1 pointer结构， 当然有些B+树只会在叶子节点存储数据， 然后子节点之间用链表和双向链表来提升查询效率）.\nleaf node的split.\nleaf node的merge\nnon-leaf node的split\nnon-leaf node的merge 上面也看到了， 树的merge和split是比较耗时的操作(以split为例， 在某些场景下是需要一直向上递归的)， 特别在使用磁盘的场景下， 那么减少这种操作（调度）， 或者该表数据大小（压缩）通常会是优化数据库性能的重要方向\n第三章 File Formats 基于磁盘存储的数据库， 它的主要的存储单元是Page, 大小一般是4kb-16kb之间， 基本上一个Page对应一个B-Tree的节点\n原始的Page结构 p： 指针， 指向子节点 k: key v： value 这种结构非常适合用来存储大小固定的数据， 比如char(13)来存储电话号码(那么所有电话号码都是同样长度的， 这种方式的存储和查找非常的便利) 但是这种方式最大的问题是不好存储存储可变长度的数据， 比如string或者text\nslotted page 现代数据库的page结构， 主要采用的是slotted page的形式： 好处就是解决了原始Page结构不好存储可变长度记录的问题， 同时支持在数据被删除的时候进行更好的再利用（下面会提及）\nCell cell存储各种类型的数据，它在Slotted Page中的插入方式是从尾部往前插入， 然后在Header中的Poitner会依次保留各个cell的offet信息\n为什么采用这样的插入顺序其实也很好理解， 如果直接从左至右， 每次有一个新的cell, 那么原来存储在最左侧（紧挨着Header）的cell， 由于Header变化了， 它也需要相应地右移， 而从尾部向前则不需要\n当然如果要保持Offset的逻辑顺序(比如根据Cell数据的字母序)， 我们可以调整Offset的位置 插入顺序： Tom -\u0026gt; Lesile -\u0026gt; Ron， Offsets的顺序则是Lesile -\u0026gt; Ron -\u0026gt; Tom\n前面提到了， Slotted Page在空间再利用（reclaim space， 利用被删除的数据）的高效性， 以sqlite db为例， 数据库会维持一份可以用space的指针列表. 当要插入新的数据的时候， 会使用First fit或者Best fit算法去找到哪一块空间最最适合插入新数据\nFirst fit: 找到第一块可以容纳新数据的space Best fit：找到浪费最小的space(就是space和新插入数据大小的差值最小) 从空间利用率上Best fit会更好，当然First fit速度明显会更快\n假设， 没有办法找到任意一块能够容纳新数据的空白space， 并且我的空白space总和又大于新数据， 那这个时候就需要进行去碎片化(defragmenting)\nCell的结构 cell有两种， 第一种叫做Key Cell， 主要用来存储key。第二种是Key-Value Cell, 就是同时用来存储key和vlaue的， 一个page上面， 通常是这有这两种key中的任意一种的(因为两种key的结构略有不同， 所以从效率角度， 肯定是存储一种类型的会更好).\nKey Cell结构：\ncell type key size child page id key bytes Key-Value Cell结构:\ncell type key size value size key bytes Data record bytes(也就是value bytes) 第四章 Implementing B-Trees Page Header page header会存储一些和page相关的元信息， 除了上前面提到的cell offsets, 还会有诸如：\nMagic numbers sibling links rightmost pointers node high key magic numbers是会存储在Page上的的特定位置的二进制的值， 比如存储第51 ， 47， 41， 45位的值， 目的是为了验证page有没有被污染， 只要在读数据的时候， 比较一下这几位的实际值和存储在header中的值是否一样即可 一般来说每个node， 会存储n个key以及n+1 pointer指向这个node的子节点， 这个第n个key经常会被存储在page header 有时候， 这种方式会有一种变体就是key会饿pointer的数量对齐： 上面的K3就是high key\nOverflow pages 通常page的size是固定的， 在4-16kb之间， 过大的size不可避免的会照成空间的浪费。 但是page size的固定话， 当可变长的数据过大的时候，我们就需要有一个扩展的页去存储溢出的数据, 这些页就被称为overflow page.\n说到底， 磁盘的load reload的开销过大才需要固定size的page， 以及这种overflow page， 如果是内存， 之间原来的size 按比例扩容就好了\npage header会保留指向这些overflow page的指针（overflow page也需要保留指向原始页面的指针） 一般来说说， 一个page只会指向单一的overflow page， 如果还是放不下， 那么可以继续从overlow page出发链接新的overflow page\n二分查找在页面中应用 由于page header中可以以逻辑顺序保留cell的offsets， 那么我们完全可以对这些offset进行二分查找： 具体步骤就是， 现取这些offsets中最中间的那个， 然后找到对应的cell， 比较和查找键的大小来决定二分查找的方向是往左还是往右\n页的Split和Merge的传播 随着数据的插入和删除， B树的节点需要进行相应的拆分 \u0026amp; 合并， 这意味着子节点的父节点以也会发生变化， 而且这种变化通常是一层层向上传播的。有没有一种高效的算法去更新这种变化 ， 其中一种就是Breadcrums（面包屑）， 这个算法的名称其实很形象地指出了它的实现方式： 我们查找新的插入节点， 一定是从根节点之上而下的， 如果我们记录了这个path的话， 那么将其逆序， 不就找到了每个子节点的父节点嘛？ 比如我们要插入一个大于37的数值， 查找路径入上入所示， Breadcrumbs记录了每个insertion point的offset。 Breadcumbs的实现一般是stack\nRebalancing 通常来讲， 树的再平衡会涉及到大量的split和merge， 进而影响数据库性能。所以数据库通常会延迟split和merge， 并使用一种相对开销较少的方式去实现reblancing \u0026ndash; 平衡邻居节点之间的数据， 把有较多数据的节点的数据搬到隔壁的节点， 从而避免节点之间的split和merge\nRight-only appends 很多场景下 ， 数据库都有一个自增的键比如一个自增的id， 如果还是采用常规的自上而下的查找方式是纯粹的浪费时间。 Postgresql等数据库， 会直接比较插入的key的值和最右页（存储最大数据的page）的第一个key进行比较， 如果查要插入的key的值更大且该页仍有足够空间， 那么就直接把数据插入到这个最右页上， 避免了自上而下的查找（计算复杂度变化： LogN -\u0026gt; 常数时间，改进很大），这种方式也叫fast path\n压缩 数据的压缩会牵涉到一个很重要的指标： 压缩率:\n更高的压缩率意味着更好的空间利用率 同时意味着， 更高的时间复杂度， 因为解压缩会消耗大量的cpu资源 很明显， 这里有一个非常重要的tradeoff : 空间vs时间\n另外通常来讲， 我们不会再文件层级去压缩数据， 只会在page级别， 很明显前者会影响查询效率。\n另外压缩的方式除了和page绑定在一起， 也可以作为一种插件的形式进行， 和page的管理进行解耦（比如对整个column的数据进行压缩， 其实列式数据就是这么做的）\n第五章 Transaction Processing and Recovery Buffer managment 类似于IO系统， 数据库也会使用类似缓存页(page cache)的概念来管理访问数据的缓存。\n上图展示了B+树的节点Page在缓存池以及在磁盘上的对应关系。 Page cache机制的包括：\n将数据保留在内存 如果要的Page在内存中， 直接返回缓存页 如果需要的Page不再内存中， 且Buffer pool足够大， 会将该页存储在缓存池（page in） 如果没有足够空间来存储新的缓存页， 就需要运行某种机制淘汰（eviction）一部分缓存页（比如使用LFU, LRU， CLOCK等算法）， 然后当缓存页被淘汰的时候， 会写道磁盘以保证数据的一致性 缓存页可以看作是数据请求和磁盘之间的中间层， 除了数据的读取， 数据的变更其实也是优先发生在这个中间层， 而不是直接写入到磁盘， 在内存中的缓存页数据被修改的时候， 这个时候这个页面就会被标识脏页(dirty page), 标识和磁盘数据不同步。 当然dirty page中的数据最后一定会被写入到磁盘， 为了保证这个过程不会出现纰漏（比如由于停电等原因， 改变的数据没有被同步到磁盘， 进而影响了数据库的ACID保证）， 通常会使用一种叫做WAL（write ahead log, 预写日志）的机制来保证数据的一致性\nWAL其实就是一系列的数据变更记录， 通常会在事务成功发生之后（commit）被保存在磁盘， 由于很多条记录其实可以被合并成一条， 比如对一个账户A增加1000， 记录一次， 减少1000再记录一次， 那这两条记录就可以被抵消掉， 另外WAL虽然是基于磁盘的， 但是它是顺序写入而不是随机写入 ，所以它的写入速度也是很快的\n缓存页淘汰机制 常见的有LRU（Least Recently Used）, LFU(Least Freaquntly Used)，CLOCK。 LRU会比较侧重一个时效性， 越是被最近访问的Page原容易被缓存（最新访问的page会被放到链表的尾部， 然后链表头部的数据会被淘汰掉）， 在实践中(不只是数据库领域)， LFU通常会更常用一点， LFU相较于LRU会更侧重Page的访问频率， 低频的页会被有限淘汰\nClock机制\nclcok机制就是用一种环形链表将所有的Page组织在一起， 当页面被访问， 它就会被设置为1（上图灰色所示）， 当后台运行淘汰任务的时候， \u0026lsquo;指针\u0026rsquo;随机指向一个Page, 如果它是0（最近没有被访问）就会被淘汰掉， 也就是说它和LRU相比， 增加了一个随机性 如果对clock机制进行一定的改造， 把1和0的bit标识改成数值(被引用次数)， 然后当一个缓存页码，每次访问的时候， 它对应的数值就会+1， 淘汰指针转动的时候则-1, 如果是0就该页就会被淘汰， 那么它就被改造成了类似LFU的模式\n数据库日志 我们在前面的Buffer management的时候提到过淘汰缓存页时，就需要将数据写入磁盘， 为了保证数据的一致性， 需要用到WAL。WAL本质上是一种日志， 在某些数据库还会按照功能区分为： redo日志， undo日志， 简单来讲redo是为了数据恢复的， undo是为了将已经保存到了数据库的数据撤回的。 然后日志的类型会有两种， 一种是物理日志(pysical log)， 日志文件将会将数据直接记录在日志中； 另一种是逻辑日志(logical log)， 逻辑日志只记录了操作而没有数据。因为物理日志有数据， 所以通常会被用在redo阶段， 这样数据恢复会更快， 逻辑日志则会被用在undo阶段 这里我们说之所以会需要undo数据（写入了磁盘的数据并没有完成commit）， 写入磁盘的操作允许被发生在commit之后， 这种数据库策略通常被称为no-force/steal 策略，相对的force/no-steal则会要求数据在commit之前写入数据， 所以force策略下， 数据库在恢复的时候只要关心redo日志就好， 因为根据策略， 但凡是日志中显示commit是先需要写入磁盘的。 但是为啥数据库还会允许no-force的这种发生写盘操作在commit之后的操作存在呢， 原因也很简单， 提升tansaction的速度。 最后， 关于数据库日志还有一个概念是checkpoint， 顾名思义就是日志的记录点， 每个checkpoint都有一个对应的LSN（log sequence number, 单调增）， 目的是为了替身redo和undo效率的， 有了checkpont我自要从上一个LSN进行数据恢复操作就好了， 而不需要每次把所有脏页进行写盘\n并发控制 数据库中由于并发引起的读写异常， 其实和隔离级别（isolation level）有关， 常见的隔离级别有(从低到高)：\nRead Uncomiited, 允许事务并发读取别的没有提交的事务， 这种隔离级别下， 很容易导致脏读 Read Commited, 一个事务只能去读已经被提交了记录。但是这种隔离级别依然无法保证对相同数据同时读取（已经都是提交了的）， 能观察到相同的数据； 因为有可能在数据读取的过程中， 数据发生了改变， 导致并非读看到不一样结果 Reapetable Read，所以在上一个隔离级别的基础上， 这个级别能保证 对数据的同时访问， 获取相同的结果 Serializable， 最强的隔离级别， 意味着所有的事务都是按照时序展开， 会有最强的数据一致性保证，也就意味着最糟糕的并发性能 下图展示了各种隔离级别对应的读写异常的可能性， 可以看到Seriaizable基本是绝缘所有并发引起的读写异常， 当然代价就是牺牲并发.\n3种并发控制流派 有三种常见的并发控制流派：\nOCC， Optimistic concurrency control MVCC, Multiversion concurency control PCC, Pessimistic concurency control OCC 把事务分成三个阶段：\nRead phase Validation phase Write phase 在第一阶段， 会把所有并发的事务的dependency写入到read set（不改变数据库状态的操作）和write set中（会有副作用， 会改变数据库操作） 第二阶段， 就是根据read set和write set中的操作， 特别是有相交（冲突）部分的操作， 对那些可能会受影响的事务， 比如一个读相关的事务， 它的数据正在被另一个事务写， 那么这个读的事务就会被终止， 不然会导致脏读 第三阶段， 当然， 如果在第二阶段中没有检查到冲突，就可以顺利的提交\nMVCC 简单来讲就是给所有的事务一个单调增的事务ID， MVCC通常不会阻止你去取旧的数据， 但是通常会确保在系统里面只存在一个没有提交的事务版本（version）\n所以这种方式应该会产生脏读， 但是因为只允许耽搁未提交事务版本， 所以最终一致性还是可以保证的\nPCC PCC通常会和锁一起出现， 当然也有不需要要锁的实现方式， 其中最简单的PCC实现方式就是基于时间戳， 通常会有两个时间戳：\nmax_read_timestamp max_write_timestamp 除了写操作被允许可以在max_write_timestamp之前被执行， 其他的操作不能在另一个读写操作的最大timestamp执行 相对而言， 基于锁的实现方式会更加流行\n锁 死锁Deadlock 死锁会发生在两个进程或者说事务都在等待彼此去释放锁， 有点类似于循环引用的问题(A模块要导入b， b模块也要导入A)\n怎么去解决死锁呢， 当然一种简单的方式就是给登台加上一个timeout时间， 这样就可以避免陷入无限等待。 还有方式就是让系统去识别这种可能造成死锁的状态， 也就是一个后台程序去检查事务直接存在不存在上面的waits-for graph， 如果存在就停掉其中的一个事务。\nLatches Latch也是锁， 但是和lock不一样的点在于， lock更多的是一种更高高级的抽象概念，远离数据库内核的存储结构。 而latch， 则是更接近数据存储结构的， 它是用来解决Page层面也就是更底层的竞争问题的。 我们可你是不希望一个Page在被更高或者说要被split和merge的时候被访问， 因为这样很可能会导致我们前面提到的读异常， 所以需要Latch在更底层帮我们去控制竞争。 但是由于B树的特殊性， 我们知道子节点的merge和split很有可能会向上传播到root（当然只是可能， 并不是绝对）， 那么仅仅在某个页面上进行竞争控制是不够的。一种相对朴素的实现方法， 就是对整个访问路径上的所有Page加上Latch, 淡然这种肯定胡牺牲性能。 所以更常用的方式， 是使用Latch crabbing的一种方式：这种方式它不会一直保留整个访问路径上的page上的锁， 而是从根节点开始， 不断向下， 只要当前节点不是full的状态(意味着大概率不会被至下而上的split merge影响） 我们就打开这个锁， 所以在特点的时间节点， Latch crabing只会给很小一部分page加锁\n第六章 B-Tree Variants 如何提升基于B-Tree存储结构的数据库读写效率呢， 主要的改进方向其实就是-\u0026gt;减少访问硬盘的次数和时间（特别是减少小批量的写入）, 再这种所提到的所有改进方法都是围绕这个核心点来的\nCopy-on-write B-Tree 传统的B-树， 一般底层使用latch来解决再并发时的问题， copy-on-write直接舍弃了这种方式，直接复制会被修改的page(也就是脏页)，同时不会阻止用户访问旧数据。 当新的Page结构被构建完成的时候， 更新原有B-Tree的指针， 指向新的Page即可:\n这种方式无疑是增加了磁盘空间的消耗， 好处有:\n避免了latch的使用 读写互不影响 数据库系统也不会处在一种被污染的状态(无非就是新的页的pointer没有替换掉老的， 但是原来的树结构和数据还是完整的， 只是过时了而已) Lazy B-Trees lazy B-Tree主要时通过结合内存或者说优化page cache的方式， 减少磁盘访问。以mongodb种使用的默认存储引擎WiredTiger为例（简称WT） WT首先会保留一份磁盘种的树结构， 当然只是Index， 并没有具体的数据。当某个page首次被读取的时候，这个page的内容会复制到到对应的update buffers， 用户可以被允许直接访问update buffers中的数据（还没被写入磁盘）， 后天程序会周期性地将update buffer数据写入到磁盘取覆盖原来的Page， 如果覆盖页的size大于原来的size， 它会拆分成多个页 FD-Tree FD-Tree结合了B+tree和LSM树的特征（核心特点就是从随机磁盘写入-\u0026gt;顺序写入）。 它在最顶层（也称为L0）会维护一颗树， L0在无法存储更多key的时候， 会把key融合到下一层（不是所有key， 而是特殊的key， 也被称为fence, 这些fence会有一个指向下一层的Pointer 上图是FD-Tree的一个样例，可以看到最的顶层（L0）是一个树结构， 然后灰色的方框就是特殊的Index entry， 指向下一level的array的相同entry, 但是fence也有开那个中类型， 一种是external， 这种类型会被merge到下一层（也就是下一层会有一个相同的entry）， 另外一种是internal（比如上面的88）， 你会发现它只出现在L1没有出现在L2. FD-Tree的查找复杂度也是Log数量级\n第七章 前面一章， 对数据库优化方向主要围绕减少磁盘写入开销的来的， 以WiredTiger为例的方法， 都是通过数据的缓存管理来实现上面这目标， 还有一种优化的方向， 就是把耗时的磁盘随机写入操作转化为磁盘的顺序写入， LSM-Tree就是这种思想的集大成者。 LSM-Tree， 全称log structed merge， 这个名字本身就昭示了这种存储结构的特征：\nlog structure： 像日志一样只会在磁盘中追加写入(append only) merge， 不在磁盘中删改数据， 通过merge来应对不同版本数据的问题 LSM树由于append only的特征， 所以这种结构是非常适合写多于读的场景（吞吐量ingestion较高的场景）， 另外由于读写在结构上互不影响， 所以不会受类似B-tree的锁机制影响并发性能\nLSM的构成 Memtable(下面两个都算是memetable, 前者接受写， 后者只接受读）， 在内存中 Current memtable Fushing memtable On-disk flush target Flush tables Compacting tables Compated tabe 上图显示了LSM的整个生命周期，其中除了Current Memtable同时支持读写以外， 其他的阶段只支持读数据， 不支持数据的写入 另外有两个要点：\n数据在内存中的时候， 它是排过序的 数据在被写入磁盘的时候， 也会用的WAL， 防止数据丢失 当数据被写入磁盘之后， 内存中的数据就会消失， 那么后续相关数据的查询之恶能通过磁盘中的数据获取 LSM中的更新和删除 更新的话很简单， 只要写入新的数据就好， 新的数据会在后买你覆盖旧数据，问题在于如何删除数据。由于我们不会直接删除LSM存储在磁盘的数据， 如果一个数据同时存储在Memtable和Disk table的情况下， 我如何在不直接删除Disk table的数据来实现删除呢。 常用的方法就是为这个需要被删除的key设置一个tomestone（墓碑）， 这样在merge的时候，老的数据会被tomestone覆盖掉， 就达到了删除的效果\nLSM中的merge操作 因为数据会被存储在多个磁盘区域， 很有可能多个disk table这间会有数据冲突的问题， 即它们都保有相同key的数据， 所以需要通过merge来解决冲突 整个流程划分为一下三个步骤：\n从不同的迭代器(就是存储在不同区域的磁盘数据， 他们本身是sorted的， 所以可以支持iteration)依次获取数据 把这些候选值入队（优先队列，可以通过min heap小顶堆来实现)， 最小的那个值放入结果集 继续从被选择的迭代器中补充数据加入队列 重复上面的流程， 如果第二个流程遇到key相同的情况， 可以选择通过记录本身携带的timestampe来解决冲突\nLSM中的压缩(compact) 常用的方法有Leveled compaction和Size-tiered 这里以Leveled 为例， 位于顶层(接近0)的table会将数据不断“下沉” 到下一个level（前提是当前的level放不下更多table了）， 这个过程会对key的范围重合的table进行融合\n比如level 1 和 Level 2中的阴影部分， 它们key range 重合了， 所以需要融合。 融合的结果就是， 上面的数据， 也就是新的数据在观感上会一点点地“下沉”到下一个level。 另外， 通常来说下一层的Level会保留上一层2倍左右的大小\nLSM的读/写/空间放大 我们优化数据的目标其实就是解决写放大， 读放大以及空间放大的问题：\n读放大：需要读取多个地址来获取数据, LSM会有这个问题 写放大： 需要大量的重写操作， B树有这个问题， 在涉及到page层面的数据增删的时候， LSM的compact阶段其实也有 空间放大： LSM会有冗余数据， 所以这个很明显是有的 有一种指标RUM conjection（RUM分别代表read，update以及memeory）可以被用来比较及衡量一个数据库引擎的综合性能。我们需要了解的是， 通常来说， 解决上面中的任一一个问题， 都需要付出其他两个点中的1个或2个作为代价， 有点类似于分布式理论中的CAP理论， 你们很难同时拥有所有好处\nLSM 实现细节 SSTable(sorted string table) LSM树中的table是基于SSTable实现的\nSST也会用到hash表， 但是是一个稀疏的hash表， 只记录了部分key和它们的位置（offset）， 数据本身是以key和value连续存储在磁盘上的\nBloom filter LSM存在的一个性能瓶颈就是读放大， 就是它的数据存储在多个table， 如果存在一个key， 在所有的数据文件中并不存在， 它依然需要取读所有的文件， 从而成为性能瓶颈， 所以可以使用Bloom filter来过滤掉不存在的key， 今儿减少读放大引起的性能问题\nSkiplist 将文件顺序地存储在内存的一种方式就是使用skiplist\nUnordered LSM Storage 一一般来说lsm的数据都是以有序地形式被存储的， 当然也有并不是有序形式存储的结构\nBitcask bitcask会在内存中保留一份key的最新路径的hash表(Keydir), keydir会在数据库初始话的时候被加载到内存， 所以很自然地会导致初始化时间过长的问题， 同时由于数据是直接被追加到磁盘的， 并不是有序（也没有什么memtable）， 所以bitcask也不支持范围查询（range query)。 但是它的优势也很明显， 首先点查询非常快， 同时写入数据是直接追加的所以写入性能也很好\nWisckey wisckey会将index和数据记录分开记录（SST， key和value是存储在一起的）， 分别存储在index lsm tree和vlog files。 vlogs files类似bitcask， 是无序地， 顺序增加的日志文件index lsm tree保留了指向vLog的指针， 所以它可以保留了范围查询的优点 它的缺点是， 有序vlog没有关于数据的l生命周期的信息(liveness, 也就是数据是不是仍然是有效的)， 所以在垃圾回收的时候， 必须要遍历左侧的index tree， 增加了复杂度。 传统的lsm， 哪怕是被删除的数据， 也可以在数据压缩阶段被直接覆盖(前面提到的类似墓碑的标记)\n","date":"2023-12-29T00:00:00Z","permalink":"https://superjcd.github.io/p/database-internal%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E4%B8%8A/","title":"Database Internal阅读笔记(上)"}]