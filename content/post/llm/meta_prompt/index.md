---
title: é«˜çº§Promptä¼˜åŒ–æŠ€å·§-åŸºäºå…ƒæç¤º
date: 2024-04-30
categories:
    - LLM
tags:
    - æç¤ºå·¥ç¨‹
weight: 1       # You can add weight to some posts to override the default sorting (date descending)
---
æˆ‘ä»¬çŸ¥é“å¥½çš„promptä¼šç›´æ¥å…³ç³»åˆ°LLMåº”ç”¨çš„è´¨é‡ï¼Œé‚£ä¹ˆå¦‚ä½•é€šè¿‡æç¤ºå·¥ç¨‹è·å¾—æ›´å¥½çš„Promptä¼šæ˜¯ä¸€ä»¶éå¸¸é‡è¦ä¸”æœ‰æ„ä¹‰çš„äº‹æƒ…ã€‚  
åœ¨å®ç°æŸç§è°ƒä¼˜ä¹‹å‰ï¼Œ æˆ‘ä»¬é¦–å…ˆéœ€è¦ææ¸…æ¥šä¸€ä»¶äº‹æƒ…ï¼Œ å¦‚ä½•å»è¯„ä¼°æŸä¸€é¡¹å¯¹Promptè¿›è¡Œè°ƒä¼˜çš„å·¥ç¨‹æ˜¯ä¸æ˜¯çœŸçš„æ˜¯æœ‰æ•ˆçš„
## å®ç°å¯¹Query Engineçš„è¯„ä¼°
å®ç°çš„æ–¹å¼è¯´èµ·æ¥å¾ˆç®€å•ï¼Œä»¥QAåœºæ™¯ä¸ºä¾‹ï¼Œ  å‡å®šæˆ‘ä»¬æœ‰ä¸€ä¸ªäººä¸ºæ ‡æ³¨è¿‡çš„æ•°æ®é›†ï¼Œå®ƒåŒ…å«Questionåˆ—å’ŒAnsweråˆ—ï¼ˆè§ä¸‹è¡¨ï¼‰ï¼Œ ç„¶åæˆ‘ä»¬ç”¨LLMå›ç­”Questionï¼Œå¾—åˆ°ä¸€ä¸ªé¢„æµ‹å›ç­”ï¼Œ ç„¶åå¯¹ç…§çœŸå®Answerå’Œé¢„æµ‹Anwseræ˜¯ä¸æ˜¯ä¸€è‡´æˆ–è€…è¿‘ä¼¼ä¸€è‡´å°±å¥½äº†ã€‚

| Question | Answer |
| --- | --- |
| How old  is Biden now | Biden is 81 years old |
| .... | ... |
| .... | .... |

å½“ç„¶è¿™é‡Œçš„é—®é¢˜åœ¨äºï¼Œ **æˆ‘å¦‚ä½•å»‰ä»·åœ°è·å–ç±»ä¼¼ä¸Šé¢çš„è¿™ç§æ ¼å¼çš„æ•°æ®é›†å‘¢**ï¼Ÿè¦çŸ¥é“äººå·¥æ ‡æ³¨æ˜¯éå¸¸æ˜‚è´µçš„ã€‚   
ä¸€ä¸ªèªæ˜çš„æ–¹æ¡ˆæ˜¯: ç›¸è¾ƒäºåŸºäºé—®é¢˜ç»™å‡ºå›ç­”ï¼Œ ä½•ä¸å¦‚åŸºäºå›ç­”æ¥è·å–é—®é¢˜å‘¢ï¼Ÿåè€…åœ¨å®ç°ä¸Šï¼Œ å¯¹LLMæ¥è¯´æ˜¯æ¯”è¾ƒå®¹æ˜“çš„ï¼Œ æ‰€ä»¥åªè¦é€šè¿‡LLMæ¥è§£ææ–‡æœ¬ï¼Œ å¹¶åŸºäºæ–‡æœ¬å†…å®¹æ¥ç”±LLMè¾“å‡ºå¯¹åº”çš„é—®é¢˜ï¼Œ æˆ‘ä»¬å°±èƒ½å¤Ÿè·å¾—ä¸€ä¸ªç±»ä¼¼äºäººä¸ºæ ‡æ³¨è¿‡çš„"é»„é‡‘æ•°æ®é›†"(Golden Dataset)

### å…·ä½“ä»£ç å®ç°--ä»¥llamaindexä¸ºä¾‹å­
å‡†å¤‡
```python
from llama_index.core import Settings
from llama_index.llms.ollama import Ollama
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

llm = Ollama(model="llama3", request_timeout=60.0, temperature=0.1)
Settings.llm = llm
embed_modle =  HuggingFaceEmbedding("BAAI/bge-base-en-v1.5")
Settings.embed_model = embed_modle
```
> è€æ ·å­ï¼Œ è´«ç©·çš„æˆ‘ä¾ç„¶é€‰æ‹©llama3å¼€å±€ ğŸ¤¡ï¼ˆç†æƒ³æƒ…å†µä¸‹è‚¯å®šæ˜¯ä¸Šæœ€æ–°ç‰ˆæœ¬çš„chatgptä¼šæ›´å¥½ï¼‰

è·å–æ•°æ®:
```python
from llama_index.readers.file import PDFReade
from llama_index.core import Document

loader = PDFReader()
docs0 = loader.load_data(file="./data/llama2.pdf")
doc_text = "\n\n".join([d.get_content() for d in docs0])
docs = [Document(text=doc_text)]
```
> llama2.pdfæ˜¯llama2çš„è®ºæ–‡ï¼Œä¸‹è½½åœ°å€ï¼š[https://arxiv.org/pdf/2307.09288](https://arxiv.org/pdf/2307.09288)

```python
from llama_index.core.node_parser import SentenceSplitter

node_parser = SentenceSplitter(chunk_size=1024)
base_nodes = node_parser.get_nodes_from_documents(docs)

```
> è¿™é‡Œæˆ‘ä»¬å¯¹æ–‡æ¡£æŒ‰å¥å­è¿›è¡Œåˆ‡åˆ†


```python
from llama_index.core import VectorStoreIndex

index = VectorStoreIndex(base_nodes)

query_engine = index.as_query_engine(similarity_top_k=2)
```

ç”Ÿæˆæ¨¡æ‹Ÿçš„"é»„é‡‘æ•°æ®é›†"
```python

import nest_asyncio

nest_asyncio.apply()

dataset_generator = DatasetGenerator(
    base_nodes[:20],
    show_progress=True,
    num_questions_per_chunk=3,
)


eval_dataset = await dataset_generator.agenerate_dataset_from_nodes(num=60)
```
> nest_asyncio.apply()æ˜¯ä¸ºäº†æ–¹ä¾¿æˆ‘ä»¬åœ¨jupyterlabä¸­è¿è¡Œå¼‚æ­¥ä»£ç ï¼Œ ä¸Šé¢çš„ä¾‹å­å…¶å®ä¸éœ€è¦ï¼ˆå› ä¸ºjupyterlabæœ¬èº«å°±æ˜¯è¿è¡Œåœ¨ä¸€ä¸ªeventloopä¸Šçš„ï¼‰ï¼Œ ä½†æ˜¯åé¢ä¼šç”¨åˆ°

`eval_dataset`å°±æ˜¯æˆ‘ä»¬é€šè¿‡LLMè·å–çš„æ•°æ®é›†ï¼Œ å¦‚æœä½ è¿˜æ˜¯å¾ˆå¥½å¥‡DatasetGeneratorå®ç°åŸç†ï¼Œ ä½ åªè¦çœ‹ä¸€ä¸‹å®ƒçš„promptå°±çŸ¥é“äº†ï¼š
> å¦‚æœä½ ä¸æ„Ÿå…´è¶£çš„è¯ï¼Œ è¿™éƒ¨åˆ†å¯ä»¥è·³è¿‡

é¦–å…ˆæˆ‘ä»¬å‡†å¤‡ä¸€ä¸ªå¯ä»¥ä¼˜ç¾å±•ç¤ºpromptçš„è¾…åŠ©å‡½æ•°ï¼š
```python
from IPython.display import Markdown, display

def display_prompt_dict(prompts_dict):
    for k, p in prompts_dict.items():
        text_md = f"**Prompt Key**: {k}<br>" f"**Text:** <br>"
        display(Markdown(text_md))
        print(p.get_template())
        display(Markdown("<br><br>"))
```
ç„¶åæŸ¥çœ‹dataset_generatorçš„prompt:
```python
display_prompt_dict(dataset_generator.get_prompts())
```
è¾“å‡ºï¼š
```
Context information is below.
---------------------
{context_str}
---------------------
Given the context information and not prior knowledge.
generate only questions based on the below query.
{query_str}
```
Seeï¼Ÿæˆ‘ä»¬å…¶å®æ˜¯è®©å¤§æ¨¡å‹æ ¹æ®æ–‡æœ¬æ¥ç”Ÿæˆé—®é¢˜

## ä½¿ç”¨Meta Promptæ¥è°ƒä¼˜prompt
é¦–å…ˆè¿™ä¸ªideaæ¥è‡ªäºè¿™ç¯‡è®ºæ–‡ï¼š[https://arxiv.org/pdf/2309.03409](https://arxiv.org/pdf/2309.03409)  
è‡³äºå®ç°ï¼Œ ç®€å•æ¥è®²ï¼Œ å°±æ˜¯é€šè¿‡è¿­ä»£çš„æ–¹å¼ï¼Œ è®©Meta Prompt(å…ƒæç¤º)åŸºäºç°æœ‰çš„promptè¡¨ç°(æ˜¯ä¸€ä¸ªéšç€è¿­ä»£ä¸æ–­æ‰©å±•çš„åˆ—è¡¨ï¼Œ å®ƒä¼šæˆä¸ºMeta Promptçš„ä¸€éƒ¨åˆ†æ¥åšä¸ºä¸Šä¸‹æ–‡çš„ä¸€éƒ¨åˆ†)ï¼Œ æ¥ç”Ÿæˆæ›´å¥½çš„æç¤ºæ¥æ”¹å–„æˆ‘ä»¬çš„åˆå§‹prompt  
é¦–å…ˆæˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹æˆ‘ä»¬çš„å…ƒæç¤ºé•¿ä»€ä¹ˆæ ·ï¼š  
```python
meta_tmpl_str = """\
Your task is to generate the instruction <INS>. Below are some previous instructions with their scores.
The score ranges from 1 to 5.

{prev_instruction_score_pairs}

Below we show the task. The <INS> tag is prepended to the below prompt template, e.g. as follows:

######
<INS>
{prompt_tmpl_str}
#######

The prompt template contains template variables. Given an input set of template variables, the formatted prompt is then given to an LLM to get an output.

Some examples of template variable inputs and expected outputs are given below to illustrate the task. **NOTE**: These do NOT represent the \
entire evaluation dataset.

{qa_pairs_str}

We run every input in an evaluation dataset through an LLM. If the LLM-generated output doesn't match the expected output, we mark it as wrong (score 0).
A correct answer has a score of 1. The final "score" for an instruction is the average of scores across an evaluation dataset.
Write your new instruction (<INS>) that is different from the old ones and has a score as high as possible.

Instruction (<INS>): \
"""
```
è¿™ä¸€é•¿ä¸²çš„promtçš„æœ€ç»ˆç›®çš„æ˜¯ä¸ºäº†ç”Ÿæˆä¸€ä¸ªpromtçš„å‰ç¼€ï¼ˆä¹Ÿå°±æ˜¯ä¸Šé¢å…ƒæç¤ºä¸­çš„Instruction (<INS>)ï¼‰ï¼Œ åç»­ä¼šä¸æ–­åœ°æ ¹æ®ç”Ÿæˆçš„InstructionåŠç›¸åº”çš„è¡¨ç°ï¼ˆè¿™ä¸ªè¡¨ç°å°±æ˜¯åŸºäºå‰é¢æˆ‘ä»¬æ¨¡æ‹Ÿç”Ÿæˆçš„é‚£ä¸ªæ•°æ®é›†æ¥çš„ï¼‰

é¦–å…ˆæˆ‘ä»¬éœ€è¦å‡†å¤‡å¥½ç”¨æ¥è¯„ä¼°LLMé—®ç­”æ•ˆæœçš„å‡½æ•°ï¼š
```python
from llama_index.core.evaluation.eval_utils import get_responses 
from llama_index.core.evaluation import CorrectnessEvaluator, BatchEvalRunner

evaluator_c = CorrectnessEvaluator()
evaluator_dict = {
    "correctness": evaluator_c,
}
batch_runner = BatchEvalRunner(evaluator_dict, workers=2, show_progress=True)


async def get_correctness(query_engine, eval_qa_pairs, batch_runner):
    eval_qs = [q for q, _ in eval_qa_pairs]
    eval_answers = [a for _, a in eval_qa_pairs]
    pred_responses = get_responses(eval_qs, query_engine, show_progress=True)

    eval_results = await batch_runner.aevaluate_responses(
        eval_qs, responses=pred_responses, reference=eval_answers
    )
    avg_correctness = np.array(
        [r.score for r in eval_results["correctness"]]
    ).mean()
    return avg_correctness
```
å¯ä»¥çœ‹åˆ°`get_correctness`ä¼šé€šè¿‡æ–°çš„query_engine(å†…åµŒäº†æ–°ç”Ÿæˆçš„æç¤ºå‰ç¼€, ä¹Ÿå°±æ˜¯Instruction)æ¥å¯¹æˆ‘ä»¬çš„æ¨¡æ‹Ÿæ•°æ®ä¸­çš„é—®é¢˜è¿›è¡Œå›ç­”ï¼Œ ç„¶ååŸºäºè¿™ä¸ªé¢„æµ‹å›ç­”å’Œæ¨¡æ‹Ÿæ•°æ®ä¸­çš„å›ç­”æ¥è·å¾—å‡†ç¡®ç‡
```python
from llama_index.core import PromptTemplate

QA_PROMPT_KEY = "response_synthesizer:text_qa_template"

meta_tmpl = PromptTemplate(meta_tmpl_str)
```
```python
def format_meta_tmpl(
    prev_instr_score_pairs,
    prompt_tmpl_str,
    qa_pairs,
    meta_tmpl,
):
    """Call meta-prompt to generate new instruction."""
    # format prev instruction score pairs.
    pair_str_list = [
        f"Instruction ():\n{instr}\nScore:\n{score}"
        for instr, score in prev_instr_score_pairs
    ]
    full_instr_pair_str = "\n\n".join(pair_str_list)

    # now show QA pairs with ground-truth answers
    qa_str_list = [
        f"query_str:\n{query_str}\nAnswer:\n{answer}"
        for query_str, answer in qa_pairs
    ]
    full_qa_pair_str = "\n\n".join(qa_str_list)

    fmt_meta_tmpl = meta_tmpl.format(
        prev_instruction_score_pairs=full_instr_pair_str,
        prompt_tmpl_str=prompt_tmpl_str,
        qa_pairs_str=full_qa_pair_str,
    )
    return fmt_meta_tmpl
```
`prev_instr_score_pairs`æ˜¯ä¹‹å‰è¿­ä»£äº§å‡ºçš„æç¤ºå‰ç¼€ä»¥åŠç›¸åº”çš„è¯„åˆ†ï¼Œ æ‰€ä»¥å®ƒä¼šéšç€è¿­ä»£çš„å¢åŠ è€Œä¸æ–­æ‰©å±•ï¼Œ å®ƒçš„é•¿åº¦ä¼šå’Œè¿­ä»£æ¬¡æ•°å¯¹åº”çš„
```python
def get_full_prompt_template(cur_instr: str, prompt_tmpl):
    tmpl_str = prompt_tmpl.get_template()
    new_tmpl_str = cur_instr + "\n" + tmpl_str
    new_tmpl = PromptTemplate(new_tmpl_str)
    return new_tmpl
```
`get_full_prompt_template`å…¶å®å°±æ˜¯æŠŠæˆ‘ä»¬ç”Ÿæˆçš„Intructionå’ŒåŸæœ‰çš„æç¤ºè¿›è¡Œäº†æ‹¼æ¥ï¼Œ åç»­ä¼šæ‹¿è¿™ä¸ªæ‹¼æ¥å®Œçš„æç¤ºå»æ›´æ–°Intruction  

æœ€åå°±æ˜¯æˆ‘ä»¬çš„ä¸»æµç¨‹ä»£ç ï¼š  
```python
def _parse_meta_response(meta_response: str):
    return str(meta_response).split("\n")[0]


async def optimize_prompts(
    query_engine,
    initial_instr: str,
    base_prompt_tmpl,
    meta_tmpl,
    meta_llm,
    batch_eval_runner,
    eval_qa_pairs,
    exemplar_qa_pairs,
    num_iterations: int = 5,
):
    prev_instr_score_pairs = []
    base_prompt_tmpl_str = base_prompt_tmpl.get_template()

    cur_instr = initial_instr
    for idx in range(num_iterations):
        if idx > 0:
            # first generate
            fmt_meta_tmpl = format_meta_tmpl(
                prev_instr_score_pairs,
                base_prompt_tmpl_str,
                exemplar_qa_pairs,
                meta_tmpl,
            )
            meta_response = meta_llm.complete(fmt_meta_tmpl)
            print(fmt_meta_tmpl)
            print(str(meta_response))
            # Parse meta response
            cur_instr = _parse_meta_response(meta_response)

        # append instruction to template
        new_prompt_tmpl = get_full_prompt_template(cur_instr, base_prompt_tmpl)
        query_engine.update_prompts({QA_PROMPT_KEY: new_prompt_tmpl})

        avg_correctness = await get_correctness(
            query_engine, eval_qa_pairs, batch_runner
        )
        prev_instr_score_pairs.append((cur_instr, avg_correctness))

    # find the instruction with the highest score
    max_instr_score_pair = max(
        prev_instr_score_pairs, key=lambda item: item[1]
    )

    # return the instruction
    return max_instr_score_pair[0], prev_instr_score_pairs
```

ç„¶åæ­£å¼åœ°å¼€å§‹è¿­ä»£
```python
new_instr, prev_instr_score_pairs = await optimize_prompts(
    query_engine,
    initial_instr,
    base_qa_prompt,
    meta_tmpl,
    llm,  # note: treat llm as meta_llm
    batch_runner,
    eval_qr_pairs,
    exemplar_qr_pairs,
    num_iterations=5,
)


new_qa_prompt = query_engine.get_prompts()[QA_PROMPT_KEY]
print(new_qa_prompt)
```
æœ€åè¾“å‡ºçš„new_qa_promptåº”è¯¥å°±æ˜¯è¯„åˆ†æœ€å¥½çš„é‚£ä¸ªpromptäº†ï¼Œ å½“ç„¶æˆ‘ä»¬ä¹Ÿå¯ä»¥æ‰“å°prev_instr_score_pairsï¼Œçœ‹çœ‹å®ƒæ˜¯ä¸æ˜¯æœ€å¥½çš„prompt

## å‚è€ƒ
[https://docs.llamaindex.ai/en/stable/examples/prompts/prompt_optimization](https://docs.llamaindex.ai/en/stable/examples/prompts/prompt_optimization/)


